{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b454eba1-b23c-4fe5-a6c3-496ac328176b",
   "metadata": {},
   "source": [
    "# Imports + utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecccb89c-547c-4de2-a09e-e04e36678931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 13:20:29.432013: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-12 13:20:29.443757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768242029.458414 1723703 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768242029.462867 1723703 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768242029.473361 1723703 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768242029.473373 1723703 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768242029.473374 1723703 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768242029.473376 1723703 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-12 13:20:29.476900: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383f7d5-ed64-40cf-b975-5f505e53ad2b",
   "metadata": {},
   "source": [
    "# Label spaces + mapping to union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f640ef9b-ba6d-4060-8d28-a7d66625740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-specific label orders (MUST match how each model was trained)\n",
    "BABY2020_LABELS = [\"Sleepy\", \"Hungry\", \"Wakeup\"]\n",
    "CHINESE_LABELS  = [\"Diaper\", \"Uncomfortable\", \"Sleepy\"]\n",
    "\n",
    "# Union label space (your algorithm)\n",
    "UNION_LABELS    = [\"Diaper\", \"Uncomfortable\", \"Sleepy\", \"Hungry\", \"Wakeup\"]\n",
    "\n",
    "# Index maps\n",
    "baby2idx   = {c:i for i,c in enumerate(BABY2020_LABELS)}\n",
    "china2idx  = {c:i for i,c in enumerate(CHINESE_LABELS)}\n",
    "union2idx  = {c:i for i,c in enumerate(UNION_LABELS)}\n",
    "idx2union  = {i:c for c,i in union2idx.items()}\n",
    "\n",
    "BABY_SLEEPY_IDX  = baby2idx[\"Sleepy\"]\n",
    "CHINA_SLEEPY_IDX = china2idx[\"Sleepy\"]\n",
    "UNION_SLEEPY_IDX = union2idx[\"Sleepy\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d236d16-b605-4220-82e7-acf57592bb34",
   "metadata": {},
   "source": [
    "# Define model classes (placeholders)\n",
    "\n",
    "## real CNN+LMU classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c29ca84-8886-44aa-b50e-7031907f8915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'saved_models/best_val_f1_score_epoch510_f10.6540_20251216_140443.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1723703/426183096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mCHINESE_CKPT_PATH\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34mr\"saved_models/model_20251231_205029_epoch626_f10.7251.h5\"\u001b[0m \u001b[0;31m# r\"saved_models/Chinese babycry stft f0 mfcc model_20251204_201256_epoch303_f10.8516.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mbaby_model\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBABY2020_CKPT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mchina_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHINESE_CKPT_PATH\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'saved_models/best_val_f1_score_epoch510_f10.6540_20251216_140443.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# ----------------------------\n",
    "# 0) (Optional) Safety toggles\n",
    "# ----------------------------\n",
    "# If you already set these earlier, you can skip this block.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Label spaces (keep consistent with training)\n",
    "# ----------------------------\n",
    "BABY2020_LABELS = [\"Sleepy\", \"Hungry\", \"Wakeup\"]\n",
    "CHINESE_LABELS  = [\"Diaper\", \"Uncomfortable\", \"Sleepy\"]\n",
    "UNION_LABELS    = [\"Diaper\", \"Uncomfortable\", \"Sleepy\", \"Hungry\", \"Wakeup\"]\n",
    "\n",
    "# Classes_chinese: {0: 'Awake', 1: 'Diaper', 2: 'Hug', 3: 'Hungry', 4: 'Sleepy', 5: 'Uncomfortable'}\n",
    "# CHINESE_LABELS = [\"Wakeup\", \"Diaper\", \"Hug\", \"Hungry\", \"Sleepy\", \"Uncomfortable\"]\n",
    "# UNION_LABELS = [\"Wakeup\", \"Diaper\", \"Hug\", \"Hungry\", \"Sleepy\", \"Uncomfortable\"]\n",
    "# ----------------------------\n",
    "# 2) Your architecture builder (same as your training)\n",
    "#    (Keep it here only if you want to re-train. For loading .h5 you don't need it.)\n",
    "# ----------------------------\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Dropout, TimeDistributed, Flatten, LSTM, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    l2_reg = regularizers.l2(0.001)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", kernel_regularizer=l2_reg)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\", kernel_regularizer=l2_reg)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\", kernel_regularizer=l2_reg)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    x = LSTM(32, return_sequences=False, kernel_regularizer=l2_reg)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(64, activation=\"relu\", kernel_regularizer=l2_reg)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation=\"softmax\", kernel_regularizer=l2_reg)(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=CategoricalCrossentropy(label_smoothing=0.1),\n",
    "        metrics=[\"accuracy\"],\n",
    "        jit_compile=False\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Load your trained .h5 models\n",
    "# ----------------------------\n",
    "BABY2020_CKPT_PATH = r\"saved_models/best_val_f1_score_epoch510_f10.6540_20251216_140443.h5\"\n",
    "CHINESE_CKPT_PATH  = r\"saved_models/model_20251231_205029_epoch626_f10.7251.h5\" # r\"saved_models/Chinese babycry stft f0 mfcc model_20251204_201256_epoch303_f10.8516.h5\"\n",
    "\n",
    "baby_model  = tf.keras.models.load_model(BABY2020_CKPT_PATH, compile=False)\n",
    "china_model = tf.keras.models.load_model(CHINESE_CKPT_PATH,  compile=False)\n",
    "\n",
    "print(\"✅ Loaded Keras models\")\n",
    "print(\"Baby output shape:\", baby_model.output_shape)\n",
    "print(\"China output shape:\", china_model.output_shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) IMPORTANT: get logits (pre-softmax) for temperature scaling\n",
    "#    Your saved model ends with softmax, so we build a logits-model.\n",
    "# ----------------------------\n",
    "def make_logits_model(softmax_model: tf.keras.Model) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a new model with SAME input, but output = pre-softmax logits.\n",
    "    Works when final layer is Dense(., activation='softmax').\n",
    "    \"\"\"\n",
    "    last = softmax_model.layers[-1]\n",
    "\n",
    "    # Case A: last layer is Dense with softmax activation\n",
    "    if isinstance(last, tf.keras.layers.Dense) and last.activation == tf.keras.activations.softmax:\n",
    "        # Build a Dense layer with same weights but linear activation\n",
    "        logits_layer = tf.keras.layers.Dense(\n",
    "            units=last.units,\n",
    "            activation=None,\n",
    "            use_bias=last.use_bias,\n",
    "            name=last.name + \"_logits\"\n",
    "        )\n",
    "\n",
    "        # Create model graph up to penultimate layer\n",
    "        penultimate_out = softmax_model.layers[-2].output\n",
    "        logits_out = logits_layer(penultimate_out)\n",
    "\n",
    "        logits_model = tf.keras.Model(inputs=softmax_model.input, outputs=logits_out, name=softmax_model.name + \"_logits\")\n",
    "\n",
    "        # Copy weights from softmax Dense -> linear Dense\n",
    "        logits_layer.set_weights(last.get_weights())\n",
    "        return logits_model\n",
    "\n",
    "    # Case B: last layer isn't a softmax Dense (maybe you already output logits)\n",
    "    # We'll try to detect if output already sums to 1; if not, treat as logits.\n",
    "    return softmax_model\n",
    "\n",
    "baby_logits_model  = make_logits_model(baby_model)\n",
    "china_logits_model = make_logits_model(china_model)\n",
    "\n",
    "print(\"✅ Built logits wrappers\")\n",
    "print(\"Baby logits output:\", baby_logits_model.output_shape)\n",
    "print(\"China logits output:\", china_logits_model.output_shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Helper: run logits + probs with temperature (for fusion later)\n",
    "# ----------------------------\n",
    "def tf_logits_and_probs(logits_model, x_np, T=1.0):\n",
    "    \"\"\"\n",
    "    x_np: numpy batch\n",
    "    returns:\n",
    "      z: [B,C] logits (numpy)\n",
    "      p: [B,C] probs after temperature (numpy)\n",
    "    \"\"\"\n",
    "    z = logits_model(x_np, training=False).numpy()\n",
    "    zT = z / float(T)\n",
    "    p = tf.nn.softmax(zT, axis=-1).numpy()\n",
    "    return z, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a86700-5cc4-4bc7-b9dc-2047c20a9bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Model: Baby2020\n",
      "Input shape : (None, 100, 280, 1)\n",
      "Output shape: (None, 3)\n",
      "Last layer  : dense_11 Dense\n",
      "Dense units : 3\n",
      "Activation : <function softmax at 0x14c12a332710>\n",
      "\n",
      "============================================================\n",
      "Model: Chinese\n",
      "Input shape : (None, 100, 280, 1)\n",
      "Output shape: (None, 3)\n",
      "Last layer  : dense_1 Dense\n",
      "Dense units : 3\n",
      "Activation : <function softmax at 0x14c12a332710>\n"
     ]
    }
   ],
   "source": [
    "def inspect_model(model, name):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(\"Input shape :\", model.input_shape)\n",
    "    print(\"Output shape:\", model.output_shape)\n",
    "    print(\"Last layer  :\", model.layers[-1].name, type(model.layers[-1]).__name__)\n",
    "    \n",
    "    # If last layer is Dense, show units + activation\n",
    "    last = model.layers[-1]\n",
    "    if hasattr(last, \"units\"):\n",
    "        print(\"Dense units :\", last.units)\n",
    "    if hasattr(last, \"activation\"):\n",
    "        print(\"Activation :\", last.activation)\n",
    "\n",
    "inspect_model(baby_model, \"Baby2020\")\n",
    "inspect_model(china_model, \"Chinese\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b7e6f2-8887-47cd-b8e6-e90a3374d807",
   "metadata": {},
   "source": [
    "# Load Test\n",
    "\n",
    "### Feaure set for test for Chinese babycry and Baby2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee19477-2482-4783-acd9-9e30ee45fe36",
   "metadata": {},
   "source": [
    "# Combined feature module for Feature extration modular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aece1896-3717-4bf8-99d0-78a02846672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple, Dict, Optional, Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "import librosa\n",
    "\n",
    "# -----------------------------\n",
    "# General parameters (can be overridden per call)\n",
    "# -----------------------------\n",
    "AUDIO_EXTS = (\".wav\", \".mp3\", \".flac\", \".ogg\", \".m4a\")\n",
    "\n",
    "def _is_audio(p: Path) -> bool:\n",
    "    return p.suffix.lower() in AUDIO_EXTS\n",
    "\n",
    "def _interp_resize_2d(feat_2d: np.ndarray, target_len: int) -> np.ndarray:\n",
    "    \"\"\"Resize a (C, T) feature matrix along time axis to target_len using linear interpolation.\"\"\"\n",
    "    feat_2d = np.asarray(feat_2d)\n",
    "    assert feat_2d.ndim == 2, f\"Expected 2D (C,T), got {feat_2d.shape}\"\n",
    "    C, T = feat_2d.shape\n",
    "    if T == target_len:\n",
    "        return feat_2d.astype(np.float32, copy=False)\n",
    "\n",
    "    orig_idx = np.linspace(0.0, 1.0, num=T, endpoint=True)\n",
    "    tgt_idx  = np.linspace(0.0, 1.0, num=target_len, endpoint=True)\n",
    "    out = np.empty((C, target_len), dtype=np.float32)\n",
    "    for c in range(C):\n",
    "        f = interp1d(orig_idx, feat_2d[c, :], kind=\"linear\", assume_sorted=True)\n",
    "        out[c, :] = f(tgt_idx)\n",
    "    return out\n",
    "\n",
    "def _choose_target_len(lengths: Iterable[int], policy: Literal[\"median\",\"max\"]=\"median\") -> int:\n",
    "    arr = np.array(list(lengths), dtype=int)\n",
    "    if len(arr) == 0:\n",
    "        raise ValueError(\"Cannot choose target length: empty lengths.\")\n",
    "    return int(np.median(arr)) if policy == \"median\" else int(arr.max())\n",
    "\n",
    "def _stft(\n",
    "    y: np.ndarray, sr: int,\n",
    "    n_fft: int, win_length: int, hop_length: int,\n",
    "    power: float = 1.0, to_db: bool = True\n",
    ") -> np.ndarray:\n",
    "    S_complex = librosa.stft(\n",
    "        y, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "        window='hann', center=True\n",
    "    )\n",
    "    S_mag = np.abs(S_complex) ** power\n",
    "    if to_db:\n",
    "        # librosa uses 10*log10 for power and 20*log10 for amplitude internally\n",
    "        S_db = librosa.power_to_db(S_mag, ref=np.max) if power != 1.0 else librosa.amplitude_to_db(S_mag, ref=np.max)\n",
    "        return S_db.astype(np.float32)\n",
    "    return S_mag.astype(np.float32)\n",
    "\n",
    "def _mfcc(\n",
    "    y: np.ndarray, sr: int,\n",
    "    n_mfcc: int, n_fft: int, win_length: int, hop_length: int\n",
    ") -> np.ndarray:\n",
    "    M = librosa.feature.mfcc(\n",
    "        y=y, sr=sr, n_mfcc=n_mfcc,\n",
    "        n_fft=n_fft, hop_length=hop_length, win_length=win_length, center=True\n",
    "    )\n",
    "    return M.astype(np.float32)\n",
    "\n",
    "def _load_audio(path: Path, sr: int) -> np.ndarray:\n",
    "    y, _ = librosa.load(str(path), sr=sr, mono=True)\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "def _scan_pairs(\n",
    "    f0_dir: Path, audio_dir: Path\n",
    ") -> List[Tuple[str, str, Path, Path]]:\n",
    "    \"\"\"\n",
    "    Pair items by (class_folder, file stem). Returns list of tuples:\n",
    "      (class_label, stem, f0_npy_path, audio_path)\n",
    "\n",
    "    - f0_dir structure: f0_dir/<class>/*_f0_wave_conf.npy (or any .npy)\n",
    "    - audio_dir structure: audio_dir/<class>/*.(wav|mp3|...)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    if not f0_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing F0 dir: {f0_dir}\")\n",
    "    if not audio_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing audio dir: {audio_dir}\")\n",
    "\n",
    "    # index audio by (class, stem) -> path\n",
    "    audio_index: Dict[Tuple[str,str], Path] = {}\n",
    "    for class_dir in sorted([d for d in audio_dir.iterdir() if d.is_dir()]):\n",
    "        cls = class_dir.name\n",
    "        for ap in class_dir.rglob(\"*\"):\n",
    "            if ap.is_file() and _is_audio(ap):\n",
    "                audio_index[(cls, ap.stem)] = ap\n",
    "\n",
    "    # walk f0 npy files and find matching audio\n",
    "    for class_dir in sorted([d for d in f0_dir.iterdir() if d.is_dir()]):\n",
    "        cls = class_dir.name\n",
    "        for npy in class_dir.rglob(\"*.npy\"):\n",
    "            stem = npy.stem\n",
    "            # allow suffixes like *_f0_wave_conf; use split at first suffix\n",
    "            stem_clean = stem.replace(\"_f0_wave_conf\", \"\")\n",
    "            key = (cls, stem_clean)\n",
    "            if key not in audio_index:\n",
    "                # fallback: try exact stem\n",
    "                if (cls, stem) in audio_index:\n",
    "                    key = (cls, stem)\n",
    "                else:\n",
    "                    # couldn't match — skip silently but could warn\n",
    "                    # print(f\"[WARN] No matching audio for {npy}\")\n",
    "                    continue\n",
    "            pairs.append((cls, key[1], npy, audio_index[key]))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def _load_f0_array(npy_path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expect shape (3, T): [wave_on_grid; f0_hz; confidence].\n",
    "    \"\"\"\n",
    "    arr = np.load(str(npy_path))\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim != 2 or arr.shape[0] != 3:\n",
    "        raise ValueError(f\"Expected F0 array shape (3, T). Got {arr.shape} from {npy_path}\")\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def build_split(\n",
    "    f0_dir: str,\n",
    "    audio_dir: str,\n",
    "    *,\n",
    "    sr: int = 16000,\n",
    "    frame_ms: float = 30.0,\n",
    "    hop_ms: float = 15.0,\n",
    "    n_mfcc: int = 20,\n",
    "    stft_power: float = 1.0,\n",
    "    stft_to_db: bool = True,\n",
    "    fixed_target_len: int | None = None,   # <— NEW\n",
    "    target_len_policy: Literal[\"median\",\"max\"] = \"median\",\n",
    "    modalities: Iterable[Literal[\"stft\",\"mfcc\",\"f0\"]] = (\"stft\",\"mfcc\",\"f0\"),\n",
    "    strict_triplet: bool = True,\n",
    "    min_frames: int = 2,                         # <— NEW\n",
    ") -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Build a single split (train OR test).\n",
    "\n",
    "    Returns:\n",
    "      X  : (N, T, F) float32 — concatenated [modalities] along feature dim\n",
    "      y  : (N,) int labels (encoded by alphabetical class order)\n",
    "      df : manifest with columns [class, stem, f0_path, audio_path, T_stft, T_mfcc, T_f0, T_final]\n",
    "\n",
    "    Notes:\n",
    "      - Only items where **all requested modalities** exist are kept.\n",
    "      - Time axis is resized to a **single target length** chosen from STFT (if present),\n",
    "        else MFCC, else F0 — according to `target_len_policy`.\n",
    "    \"\"\"\n",
    "    f0_dir = Path(f0_dir)\n",
    "    audio_dir = Path(audio_dir)\n",
    "    pairs = _scan_pairs(f0_dir, audio_dir)\n",
    "\n",
    "    if len(pairs) == 0:\n",
    "        raise ValueError(f\"No (F0, audio) pairs found between {f0_dir} and {audio_dir}.\")\n",
    "\n",
    "    # window sizes\n",
    "    win_length = int(round(sr * frame_ms / 1000.0))\n",
    "    hop_length = int(round(sr * hop_ms   / 1000.0))\n",
    "    # next power of two for n_fft\n",
    "    n_fft = 1\n",
    "    while n_fft < win_length:\n",
    "        n_fft <<= 1\n",
    "\n",
    "    rows = []\n",
    "    features_list = []\n",
    "    labels = []\n",
    "\n",
    "    # first pass: compute raw modality features + their T lengths\n",
    "    stft_list, mfcc_list, f0_list = [], [], []\n",
    "    Ts_stft, Ts_mfcc, Ts_f0 = [], [], []\n",
    "\n",
    "    kept = 0\n",
    "    skipped_short = 0\n",
    "    for cls, stem, f0_path, audio_path in pairs:\n",
    "        try:\n",
    "            f0_arr = _load_f0_array(f0_path) if \"f0\" in modalities else None\n",
    "            y = _load_audio(audio_path, sr=sr)\n",
    "\n",
    "            stft_feat = _stft(y, sr, n_fft, win_length, hop_length,\n",
    "                              power=stft_power, to_db=stft_to_db) if \"stft\" in modalities else None\n",
    "            mfcc_feat = _mfcc(y, sr, n_mfcc, n_fft, win_length, hop_length) if \"mfcc\" in modalities else None\n",
    "\n",
    "            # record lengths\n",
    "            Ts_stft.append(stft_feat.shape[1] if stft_feat is not None else -1)\n",
    "            Ts_mfcc.append(mfcc_feat.shape[1] if mfcc_feat is not None else -1)\n",
    "            Ts_f0.append(f0_arr.shape[1]   if f0_arr   is not None else -1)\n",
    "\n",
    "            # skip if any requested modality has too few frames\n",
    "            if strict_triplet:\n",
    "                if (\"stft\" in modalities and (stft_feat is None or stft_feat.shape[1] < min_frames)) \\\n",
    "                or (\"mfcc\" in modalities and (mfcc_feat is None or mfcc_feat.shape[1] < min_frames)) \\\n",
    "                or (\"f0\"   in modalities and (f0_arr   is None or f0_arr.shape[1]   < min_frames)):\n",
    "                    skipped_short += 1\n",
    "                    continue\n",
    "\n",
    "            stft_list.append(stft_feat)\n",
    "            mfcc_list.append(mfcc_feat)\n",
    "            f0_list.append(f0_arr)\n",
    "            labels.append(cls)\n",
    "            rows.append({\n",
    "                \"class\": cls, \"stem\": stem,\n",
    "                \"f0_path\": str(f0_path), \"audio_path\": str(audio_path)\n",
    "            })\n",
    "            kept += 1\n",
    "        except Exception as e:\n",
    "            # Skip problematic files but keep going\n",
    "            # print(f\"[WARN] Skipped ({cls}/{stem}): {e}\")\n",
    "            continue\n",
    "            \n",
    "    if kept == 0:\n",
    "        raise ValueError(f\"After loading, no usable items remained (skipped_short={skipped_short}).\")\n",
    "\n",
    "    if len(labels) == 0:\n",
    "        raise ValueError(\"After loading, no usable items remained. Check data.\")\n",
    "\n",
    "    # choose target T from the first available modality in priority order\n",
    "    def _valid_lengths(L): return [x for x in L if x > 0]\n",
    "    if fixed_target_len is not None: \n",
    "        T_target = int(fixed_target_len)\n",
    "    elif \"stft\" in modalities and len(_valid_lengths(Ts_stft)) > 0:\n",
    "        T_target = _choose_target_len(_valid_lengths(Ts_stft), policy=target_len_policy)\n",
    "    elif \"mfcc\" in modalities and len(_valid_lengths(Ts_mfcc)) > 0:\n",
    "        T_target = _choose_target_len(_valid_lengths(Ts_mfcc), policy=target_len_policy)\n",
    "    elif \"f0\" in modalities and len(_valid_lengths(Ts_f0)) > 0:\n",
    "        T_target = _choose_target_len(_valid_lengths(Ts_f0), policy=target_len_policy)\n",
    "    else:\n",
    "        raise ValueError(\"Could not infer target time length from requested modalities.\")\n",
    "\n",
    "    # second pass: resize and concatenate\n",
    "    X_list = []\n",
    "    keep_mask = []\n",
    "    for stft_feat, mfcc_feat, f0_feat in zip(stft_list, mfcc_list, f0_list):\n",
    "        if strict_triplet:\n",
    "            # ensure all requested modalities exist\n",
    "            if (\"stft\" in modalities and stft_feat is None) or \\\n",
    "               (\"mfcc\" in modalities and mfcc_feat is None) or \\\n",
    "               (\"f0\"   in modalities and f0_feat   is None):\n",
    "                keep_mask.append(False)\n",
    "                X_list.append(None)\n",
    "                continue\n",
    "\n",
    "        channels = []\n",
    "        if stft_feat is not None:\n",
    "            channels.append(_interp_resize_2d(stft_feat, T_target))\n",
    "        if mfcc_feat is not None:\n",
    "            channels.append(_interp_resize_2d(mfcc_feat, T_target))\n",
    "        if f0_feat is not None:\n",
    "            channels.append(_interp_resize_2d(f0_feat, T_target))  # (3, T)\n",
    "\n",
    "        if len(channels) == 0:\n",
    "            keep_mask.append(False)\n",
    "            X_list.append(None)\n",
    "            continue\n",
    "\n",
    "        CxT = np.concatenate(channels, axis=0)     # (C_total, T)\n",
    "        X_list.append(CxT.T.astype(np.float32))    # (T, C_total)\n",
    "        keep_mask.append(True)\n",
    "\n",
    "    # filter by keep_mask\n",
    "    keep_idx = [i for i, k in enumerate(keep_mask) if k]\n",
    "    if len(keep_idx) == 0:\n",
    "        raise ValueError(\"No samples had all requested modalities (strict_triplet=True).\")\n",
    "\n",
    "    X = np.stack([X_list[i] for i in keep_idx], axis=0)   # (N, T, F)\n",
    "    y_labels = [labels[i] for i in keep_idx]\n",
    "    df = pd.DataFrame([rows[i] for i in keep_idx])\n",
    "    df[\"T_final\"] = T_target\n",
    "    if \"stft\" in modalities: df[\"T_stft\"] = [Ts_stft[i] for i in keep_idx]\n",
    "    if \"mfcc\" in modalities: df[\"T_mfcc\"] = [Ts_mfcc[i] for i in keep_idx]\n",
    "    if \"f0\"   in modalities: df[\"T_f0\"]   = [Ts_f0[i]   for i in keep_idx]\n",
    "\n",
    "    # encode labels alphabetically (stable & reproducible)\n",
    "    classes = sorted(pd.unique(df[\"class\"]))\n",
    "    cls_to_id = {c:i for i,c in enumerate(classes)}\n",
    "    y = np.array([cls_to_id[c] for c in y_labels], dtype=np.int64)\n",
    "\n",
    "    return X, y, df.assign(label_id=[cls_to_id[c] for c in y_labels])\n",
    "\n",
    "def prepare_train_test(\n",
    "    f0_conf_wave_train_dir: str,\n",
    "    f0_conf_wave_test_dir: str,\n",
    "    audio_train_dir: str,\n",
    "    audio_test_dir: str,\n",
    "    *,\n",
    "    sr: int = 16000,\n",
    "    frame_ms: float = 30.0,\n",
    "    hop_ms: float = 15.0,\n",
    "    n_mfcc: int = 20,\n",
    "    stft_power: float = 1.0,\n",
    "    stft_to_db: bool = True,\n",
    "    target_len_policy: Literal[\"median\",\"max\"] = \"median\",\n",
    "    modalities: Iterable[Literal[\"stft\",\"mfcc\",\"f0\"]] = (\"stft\",\"mfcc\",\"f0\"),\n",
    "    strict_triplet: bool = True,\n",
    "    fixed_target_len: int | None = None,   # <— NEW\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[int,str], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper to build train & test with identical settings.\n",
    "\n",
    "    Returns:\n",
    "      X_train, y_train, X_test, y_test,\n",
    "      id_to_class (dict), train_manifest (df), test_manifest (df)\n",
    "    \"\"\"\n",
    "    X_train, y_train, df_train = build_split(\n",
    "        f0_conf_wave_train_dir, audio_train_dir,\n",
    "        sr=sr, frame_ms=frame_ms, hop_ms=hop_ms, n_mfcc=n_mfcc,\n",
    "        stft_power=stft_power, stft_to_db=stft_to_db,\n",
    "        target_len_policy=target_len_policy, modalities=modalities,\n",
    "        strict_triplet=strict_triplet,\n",
    "        fixed_target_len=fixed_target_len,         # <— pass through\n",
    "        min_frames=2,                    # <— NEW\n",
    "    )\n",
    "    X_test, y_test, df_test = build_split(\n",
    "        f0_conf_wave_test_dir, audio_test_dir,\n",
    "        sr=sr, frame_ms=frame_ms, hop_ms=hop_ms, n_mfcc=n_mfcc,\n",
    "        stft_power=stft_power, stft_to_db=stft_to_db,\n",
    "        target_len_policy=target_len_policy, modalities=modalities,\n",
    "        strict_triplet=strict_triplet,\n",
    "        fixed_target_len=fixed_target_len,         # <— pass through\n",
    "        min_frames=2,                    # <— NEW\n",
    "    )\n",
    "\n",
    "    # harmonize label ids across splits (use train mapping)\n",
    "    classes = sorted(pd.unique(df_train[\"class\"]))\n",
    "    id_to_class = {i:c for i,c in enumerate(classes)}\n",
    "    cls_to_id = {c:i for i,c in id_to_class.items()}\n",
    "\n",
    "    # remap test if classes overlap; unknown classes get new ids at the end\n",
    "    test_classes = list(pd.unique(df_test[\"class\"]))\n",
    "    for c in test_classes:\n",
    "        if c not in cls_to_id:\n",
    "            cls_to_id[c] = len(cls_to_id)\n",
    "            id_to_class[cls_to_id[c]] = c\n",
    "\n",
    "    y_train = np.array([cls_to_id[c] for c in df_train[\"class\"].tolist()], dtype=np.int64)\n",
    "    y_test  = np.array([cls_to_id[c] for c in df_test[\"class\"].tolist()], dtype=np.int64)\n",
    "\n",
    "    df_train = df_train.assign(label_id=y_train)\n",
    "    df_test  = df_test.assign(label_id=y_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, id_to_class, df_train, df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb87df0-3a1e-4d17-8f4d-00739009bc0b",
   "metadata": {},
   "source": [
    "# Also do the same  the same for chinese baby cry without reverbration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d55d43-9597-44f1-9958-c4942df923b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_chinese: (734, 100, 280) X_test_cinese: (184, 100, 280)\n",
      "y_train_chinese: (734,) y_test_chinese: (184,)\n",
      "Classes_chinese: {0: 'awake', 1: 'diaper', 2: 'hug', 3: 'hungry', 4: 'sleepy', 5: 'uncomfortable'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>stem</th>\n",
       "      <th>f0_path</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>T_final</th>\n",
       "      <th>T_stft</th>\n",
       "      <th>T_mfcc</th>\n",
       "      <th>T_f0</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awake</td>\n",
       "      <td>awake_0</td>\n",
       "      <td>Chinese Babycry/Chinese baby cry train_f0/awak...</td>\n",
       "      <td>Chinese Babycry/Train_Split_80/awake/awake_0.wav</td>\n",
       "      <td>100</td>\n",
       "      <td>1051</td>\n",
       "      <td>1051</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awake</td>\n",
       "      <td>awake_100</td>\n",
       "      <td>Chinese Babycry/Chinese baby cry train_f0/awak...</td>\n",
       "      <td>Chinese Babycry/Train_Split_80/awake/awake_100...</td>\n",
       "      <td>100</td>\n",
       "      <td>1039</td>\n",
       "      <td>1039</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awake</td>\n",
       "      <td>awake_101</td>\n",
       "      <td>Chinese Babycry/Chinese baby cry train_f0/awak...</td>\n",
       "      <td>Chinese Babycry/Train_Split_80/awake/awake_101...</td>\n",
       "      <td>100</td>\n",
       "      <td>1713</td>\n",
       "      <td>1713</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>awake</td>\n",
       "      <td>awake_102</td>\n",
       "      <td>Chinese Babycry/Chinese baby cry train_f0/awak...</td>\n",
       "      <td>Chinese Babycry/Train_Split_80/awake/awake_102...</td>\n",
       "      <td>100</td>\n",
       "      <td>1148</td>\n",
       "      <td>1148</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awake</td>\n",
       "      <td>awake_103</td>\n",
       "      <td>Chinese Babycry/Chinese baby cry train_f0/awak...</td>\n",
       "      <td>Chinese Babycry/Train_Split_80/awake/awake_103...</td>\n",
       "      <td>100</td>\n",
       "      <td>1048</td>\n",
       "      <td>1048</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class       stem                                            f0_path  \\\n",
       "0  awake    awake_0  Chinese Babycry/Chinese baby cry train_f0/awak...   \n",
       "1  awake  awake_100  Chinese Babycry/Chinese baby cry train_f0/awak...   \n",
       "2  awake  awake_101  Chinese Babycry/Chinese baby cry train_f0/awak...   \n",
       "3  awake  awake_102  Chinese Babycry/Chinese baby cry train_f0/awak...   \n",
       "4  awake  awake_103  Chinese Babycry/Chinese baby cry train_f0/awak...   \n",
       "\n",
       "                                          audio_path  T_final  T_stft  T_mfcc  \\\n",
       "0   Chinese Babycry/Train_Split_80/awake/awake_0.wav      100    1051    1051   \n",
       "1  Chinese Babycry/Train_Split_80/awake/awake_100...      100    1039    1039   \n",
       "2  Chinese Babycry/Train_Split_80/awake/awake_101...      100    1713    1713   \n",
       "3  Chinese Babycry/Train_Split_80/awake/awake_102...      100    1148    1148   \n",
       "4  Chinese Babycry/Train_Split_80/awake/awake_103...      100    1048    1048   \n",
       "\n",
       "   T_f0  label_id  \n",
       "0    79         0  \n",
       "1    78         0  \n",
       "2   129         0  \n",
       "3    87         0  \n",
       "4    79         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>stem</th>\n",
       "      <th>f0_path</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>T_final</th>\n",
       "      <th>T_stft</th>\n",
       "      <th>T_mfcc</th>\n",
       "      <th>T_f0</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awake</td>\n",
       "      <td>awake_110</td>\n",
       "      <td>Chinese Babycry/Chinese baby cry test_f0/awake...</td>\n",
       "      <td>Chinese Babycry/Test_Split_20/awake/awake_110.wav</td>\n",
       "      <td>100</td>\n",
       "      <td>1292</td>\n",
       "      <td>1292</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awake</td>\n",
       "      <td>awake_113</td>\n",
       "      <td>Chinese Babycry/Chinese baby cry test_f0/awake...</td>\n",
       "      <td>Chinese Babycry/Test_Split_20/awake/awake_113.wav</td>\n",
       "      <td>100</td>\n",
       "      <td>1112</td>\n",
       "      <td>1112</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awake</td>\n",
       "      <td>awake_116</td>\n",
       "      <td>Chinese Babycry/Chinese baby cry test_f0/awake...</td>\n",
       "      <td>Chinese Babycry/Test_Split_20/awake/awake_116.wav</td>\n",
       "      <td>100</td>\n",
       "      <td>1074</td>\n",
       "      <td>1074</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>awake</td>\n",
       "      <td>awake_117</td>\n",
       "      <td>Chinese Babycry/Chinese baby cry test_f0/awake...</td>\n",
       "      <td>Chinese Babycry/Test_Split_20/awake/awake_117.wav</td>\n",
       "      <td>100</td>\n",
       "      <td>1039</td>\n",
       "      <td>1039</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awake</td>\n",
       "      <td>awake_131</td>\n",
       "      <td>Chinese Babycry/Chinese baby cry test_f0/awake...</td>\n",
       "      <td>Chinese Babycry/Test_Split_20/awake/awake_131.wav</td>\n",
       "      <td>100</td>\n",
       "      <td>1168</td>\n",
       "      <td>1168</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class       stem                                            f0_path  \\\n",
       "0  awake  awake_110  Chinese Babycry/Chinese baby cry test_f0/awake...   \n",
       "1  awake  awake_113  Chinese Babycry/Chinese baby cry test_f0/awake...   \n",
       "2  awake  awake_116  Chinese Babycry/Chinese baby cry test_f0/awake...   \n",
       "3  awake  awake_117  Chinese Babycry/Chinese baby cry test_f0/awake...   \n",
       "4  awake  awake_131  Chinese Babycry/Chinese baby cry test_f0/awake...   \n",
       "\n",
       "                                          audio_path  T_final  T_stft  T_mfcc  \\\n",
       "0  Chinese Babycry/Test_Split_20/awake/awake_110.wav      100    1292    1292   \n",
       "1  Chinese Babycry/Test_Split_20/awake/awake_113.wav      100    1112    1112   \n",
       "2  Chinese Babycry/Test_Split_20/awake/awake_116.wav      100    1074    1074   \n",
       "3  Chinese Babycry/Test_Split_20/awake/awake_117.wav      100    1039    1039   \n",
       "4  Chinese Babycry/Test_Split_20/awake/awake_131.wav      100    1168    1168   \n",
       "\n",
       "   T_f0  label_id  \n",
       "0    97         0  \n",
       "1    84         0  \n",
       "2    81         0  \n",
       "3    78         0  \n",
       "4    88         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from feature_fusion import prepare_train_test\n",
    "\n",
    "f0_conf_wave_train_chinese_dir = \"Chinese Babycry/Chinese baby cry train_f0\"\n",
    "f0_conf_wave_test_chinese_dir  = \"Chinese Babycry/Chinese baby cry test_f0\"\n",
    "\n",
    "audio_train_dir_chinese        = \"Chinese Babycry/Train_Split_80\"\n",
    "audio_test_dir_chinese         = \"Chinese Babycry/Test_Split_20\"\n",
    "\n",
    "X_train_chinese, y_train_chinese, X_test_chinese, y_test_chinese, id2cls_chinese, train_manifest_chinese, test_manifest_chinese = prepare_train_test(\n",
    "    f0_conf_wave_train_dir=f0_conf_wave_train_chinese_dir,\n",
    "    f0_conf_wave_test_dir=f0_conf_wave_test_chinese_dir,\n",
    "    audio_train_dir=audio_train_dir_chinese,\n",
    "    audio_test_dir=audio_test_dir_chinese,\n",
    "    sr=16000,\n",
    "    frame_ms=30.0,\n",
    "    hop_ms=15.0,\n",
    "    n_mfcc=20,\n",
    "    modalities=(\"stft\", \"mfcc\", \"f0\"),   # choose any subset e.g. (\"mfcc\",\"f0\")\n",
    "    fixed_target_len=100, \n",
    "    target_len_policy=\"median\",           # or \"max\"\n",
    "    strict_triplet=True,                  # require all requested modalities per sample\n",
    ")\n",
    "\n",
    "print(\"X_train_chinese:\", X_train_chinese.shape, \"X_test_cinese:\", X_test_chinese.shape)\n",
    "print(\"y_train_chinese:\", y_train_chinese.shape, \"y_test_chinese:\", y_test_chinese.shape)\n",
    "print(\"Classes_chinese:\", id2cls_chinese)\n",
    "display(train_manifest_chinese.head())\n",
    "display(test_manifest_chinese.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d9515-1eb7-45b7-b43c-09aacab36eda",
   "metadata": {},
   "source": [
    "# Filter 3 moods for Chinese baby cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8fbc47-5cd6-4688-a296-f384b0c2a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {0: 'Diaper', 1: 'Sleepy', 2: 'Uncomfortable'}\n",
      "Train shape: (350, 100, 280, 1)  Val shape: (88, 100, 280, 1)\n",
      "Unique encoded y: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Classes of interest\n",
    "keep_classes = [1, 4, 5]   # diaper=1, sleepy=4, uncomfortable=5\n",
    "class_names = {1: \"Diaper\", 4: \"Sleepy\", 5: \"Uncomfortable\"}\n",
    "\n",
    "# --- Train filtering ---\n",
    "mask_train = np.isin(y_train_chinese, keep_classes)\n",
    "X_train_split = X_train_chinese[mask_train]\n",
    "y_train_split = y_train_chinese[mask_train]\n",
    "\n",
    "# --- Test filtering ---\n",
    "mask_test = np.isin(y_test_chinese, keep_classes)\n",
    "X_val_split = X_test_chinese[mask_test]\n",
    "y_val_split = y_test_chinese[mask_test]\n",
    "\n",
    "# --- Re-encode labels to [0,1,2] ---\n",
    "unique_classes = sorted(keep_classes)  # [1,4,5]\n",
    "class2newid = {old: new for new, old in enumerate(unique_classes)}\n",
    "id2cls_merge_3mood = {new: class_names[old] for old, new in class2newid.items()}\n",
    "\n",
    "y_train_split = np.array([class2newid[y] for y in y_train_split])\n",
    "y_val_split = np.array([class2newid[y] for y in y_val_split])\n",
    "\n",
    "# --- Add channel dimension ---\n",
    "X_train_split = np.expand_dims(X_train_split, axis=-1)\n",
    "X_val_split = np.expand_dims(X_val_split, axis=-1)\n",
    "\n",
    "print(\"Class mapping:\", id2cls_merge_3mood)\n",
    "print(\"Train shape:\", X_train_split.shape, \" Val shape:\", X_val_split.shape)\n",
    "print(\"Unique encoded y:\", np.unique(y_train_split))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a62f75-0b3a-46b2-aa60-156ceba8f272",
   "metadata": {},
   "source": [
    "# Baby2020 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded373b2-75b0-4589-a5ee-afc0be29a499",
   "metadata": {},
   "source": [
    "# Also do the same (Test & Train) for baby 2020 M0 to 3 or 9 ans merge months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa7d530-0910-434f-bab3-c431f179d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _remap_labels(y_old: np.ndarray, id2cls: dict, global_cls2id: dict) -> np.ndarray:\n",
    "    return np.array([global_cls2id[id2cls[int(k)]] for k in y_old], dtype=np.int64)\n",
    "\n",
    "def _append_month_feature(X, month_idx, n_months, mode=\"onehot\"):\n",
    "    \"\"\"\n",
    "    Append month info to X along feature dim.\n",
    "    mode=\"onehot\" -> +n_months features; mode=\"index\" -> +1 feature in [0,1].\n",
    "    \"\"\"\n",
    "    N, T, F = X.shape\n",
    "    month_idx = np.asarray(month_idx, dtype=int)\n",
    "    if mode == \"onehot\":\n",
    "        X_out = np.empty((N, T, F + n_months), dtype=np.float32)\n",
    "        X_out[..., :F] = X\n",
    "        for i in range(N):\n",
    "            one = np.zeros((T, n_months), dtype=np.float32)\n",
    "            one[:, month_idx[i]] = 1.0\n",
    "            X_out[i, :, F:] = one\n",
    "        return X_out\n",
    "    elif mode == \"index\":\n",
    "        denom = max(1, n_months - 1)\n",
    "        X_out = np.empty((N, T, F + 1), dtype=np.float32)\n",
    "        X_out[..., :F] = X\n",
    "        for i in range(N):\n",
    "            X_out[i, :, F:] = float(month_idx[i]) / denom\n",
    "        return X_out\n",
    "    else:\n",
    "        return X  # no change\n",
    "\n",
    "def load_all_months_train(\n",
    "    base_path: str,\n",
    "    *,\n",
    "    months=(\"0Month\",\"1Month\",\"2Month\",\"3Month\",\"4Month\",\"5Month\",\"6Month\",\"7Month\",\"8Month\",\"9Month\"),\n",
    "    f0_root_subdir=\"Baby2020/Baby2020_f0_wave_conf_arrays\",   # sits inside base_path\n",
    "    audio_month_subdir=\"\",                           # usually empty: classes live directly under each month\n",
    "    sr=16000, frame_ms=30.0, hop_ms=15.0, n_mfcc=20,\n",
    "    modalities=(\"stft\",\"mfcc\",\"f0\"),\n",
    "    fixed_target_len=100, target_len_policy=\"median\", strict_triplet=True,\n",
    "    label_from: Literal[\"class\",\"month\"] = \"class\",\n",
    "    month_feature_mode: Literal[\"onehot\",\"index\",None] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build ONE big training set from month folders only (no test split).\n",
    "\n",
    "    Returns:\n",
    "      X_all (N, T, F[+month_feat]), y_all (N,),\n",
    "      id2cls_all (dict), df_all (manifest with month_folder), months_used (list)\n",
    "    \"\"\"\n",
    "    base = Path(base_path)\n",
    "\n",
    "    # lists to collect month-wise outputs\n",
    "    X_list, y_list = [], []\n",
    "    df_list = []\n",
    "    id2cls_per_month = []\n",
    "    class_name_set = set()\n",
    "    months_used = []\n",
    "    month_idx_list = []\n",
    "\n",
    "    # Load each month by calling your existing build_split()\n",
    "    for mi, m in enumerate(months):\n",
    "        audio_dir = base / m / audio_month_subdir\n",
    "        f0_dir    = base / f0_root_subdir / m\n",
    "\n",
    "        if not (audio_dir.exists() and f0_dir.exists()):\n",
    "            print(f\"[WARN] Skipping {m}: missing {audio_dir} or {f0_dir}\")\n",
    "            continue\n",
    "\n",
    "        # build_split expects directories with CLASS subfolders inside\n",
    "        X_m, y_m, df_m = build_split(\n",
    "            f0_dir=str(f0_dir),\n",
    "            audio_dir=str(audio_dir),\n",
    "            sr=sr, frame_ms=frame_ms, hop_ms=hop_ms, n_mfcc=n_mfcc,\n",
    "            stft_power=1.0, stft_to_db=True,\n",
    "            fixed_target_len=fixed_target_len,\n",
    "            target_len_policy=target_len_policy,\n",
    "            modalities=modalities,\n",
    "            strict_triplet=strict_triplet,\n",
    "        )\n",
    "\n",
    "        # Add month info to manifest\n",
    "        df_m = df_m.copy()\n",
    "        df_m[\"month_folder\"] = m\n",
    "        df_list.append(df_m)\n",
    "\n",
    "        # Record id2cls for this month (class mapping from build_split)\n",
    "        # and collect class names for global map\n",
    "        # If label_from=\"month\", we'll override later anyway.\n",
    "        local_classes = sorted(pd.unique(df_m[\"class\"]))\n",
    "        id2cls_m = {i: c for i, c in enumerate(local_classes)}\n",
    "        id2cls_per_month.append(id2cls_m)\n",
    "        class_name_set.update(local_classes)\n",
    "\n",
    "        X_list.append(X_m)\n",
    "        y_list.append(y_m)\n",
    "        months_used.append(m)\n",
    "        month_idx_list.append(np.full(len(y_m), mi, dtype=int))\n",
    "\n",
    "        print(f\"[OK] {m}: {X_m.shape[0]} samples, X shape {X_m.shape}\")\n",
    "        \n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"No months loaded. Check base_path and folder names.\")\n",
    "\n",
    "    # Merge across months\n",
    "    X_all = np.concatenate(X_list, axis=0)\n",
    "    df_all = pd.concat(df_list, ignore_index=True)\n",
    "    month_idx = np.concatenate(month_idx_list, axis=0)\n",
    "\n",
    "    if label_from == \"class\":\n",
    "        # Build global class mapping and remap y from each month\n",
    "        all_classes = sorted(class_name_set)\n",
    "        cls2gid = {c:i for i,c in enumerate(all_classes)}\n",
    "        id2cls_all = {i:c for c,i in cls2gid.items()}\n",
    "        y_all = np.concatenate([\n",
    "            _remap_labels(y_m, id2cls_m, cls2gid) for y_m, id2cls_m in zip(y_list, id2cls_per_month)\n",
    "        ], axis=0)\n",
    "        df_all[\"label_id\"] = y_all\n",
    "    else:\n",
    "        # Label is the month itself (0..len(months_used)-1)\n",
    "        month2id = {m:i for i,m in enumerate(months_used)}\n",
    "        y_all = np.array([month2id[m] for m in df_all[\"month_folder\"]], dtype=np.int64)\n",
    "        id2cls_all = {i:m for m,i in month2id.items()}\n",
    "        df_all[\"label_id\"] = y_all\n",
    "\n",
    "    # Optional: append month feature to X\n",
    "    if month_feature_mode in (\"onehot\",\"index\"):\n",
    "        X_all = _append_month_feature(X_all, month_idx, n_months=len(months_used), mode=month_feature_mode)\n",
    "\n",
    "    return X_all, y_all, id2cls_all, df_all, months_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c12323-33b7-4434-b2b2-8e58e60e6e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 0Month: 500 samples, X shape (500, 100, 280)\n",
      "[OK] 1Month: 550 samples, X shape (550, 100, 280)\n",
      "[OK] 2Month: 1070 samples, X shape (1070, 100, 280)\n",
      "[OK] 3Month: 1340 samples, X shape (1340, 100, 280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uottawa.o.univ/njaza024/.local/lib/python3.10/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=512 is too large for input signal of length=288\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 4Month: 1709 samples, X shape (1709, 100, 280)\n",
      "[OK] 5Month: 1650 samples, X shape (1650, 100, 280)\n",
      "[OK] 6Month: 1010 samples, X shape (1010, 100, 280)\n",
      "[OK] 7Month: 320 samples, X shape (320, 100, 280)\n",
      "[OK] 8Month: 399 samples, X shape (399, 100, 280)\n",
      "[OK] 9Month: 850 samples, X shape (850, 100, 280)\n",
      "Months loaded: ['0Month', '1Month', '2Month', '3Month', '4Month', '5Month', '6Month', '7Month', '8Month', '9Month']\n",
      "X_all: (9398, 100, 280) y_all: (9398,)\n",
      "Classes: {0: 'Hungry', 1: 'NeedHug', 2: 'Sleepy', 3: 'Temper', 4: 'UnComfy', 5: 'Uncomfy', 6: 'Wakeup'}\n",
      "    class month_folder  label_id\n",
      "0  Hungry       0Month         0\n",
      "1  Hungry       0Month         0\n",
      "2  Hungry       0Month         0\n",
      "3  Hungry       0Month         0\n",
      "4  Hungry       0Month         0\n"
     ]
    }
   ],
   "source": [
    "base = r\"Baby2020/\"  # <- use your real absolute path\n",
    "\n",
    "X_all, y_all, id2cls_all, df_all, months_used = load_all_months_train(\n",
    "    base_path=base,\n",
    "    months=(\"0Month\",\"1Month\",\"2Month\",\"3Month\",\"4Month\",\"5Month\",\"6Month\",\"7Month\",\"8Month\",\"9Month\"),\n",
    "    f0_root_subdir=\"Baby2020_f0_wave_conf_arrays\",  # under base, contains 0Month..6Month\n",
    "    audio_month_subdir=\"\",                          # classes live directly under each month folder\n",
    "    # features\n",
    "    modalities=(\"stft\",\"mfcc\",\"f0\"),\n",
    "    fixed_target_len=100,\n",
    "    # labels: choose \"class\" (default) or \"month\"\n",
    "    label_from=\"class\",\n",
    "    # optionally append month to X as features:\n",
    "    month_feature_mode=None,   # or \"onehot\" / \"index\"\n",
    ")\n",
    "\n",
    "print(\"Months loaded:\", months_used)\n",
    "print(\"X_all:\", X_all.shape, \"y_all:\", y_all.shape)\n",
    "print(\"Classes:\", id2cls_all)\n",
    "print(df_all[[\"class\",\"month_folder\",\"label_id\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90498a40-39b4-4f97-a775-4af9d6f6a9e3",
   "metadata": {},
   "source": [
    "# Make a noneleakage selection randomly test train 20 80 for baby2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbe76151-36f4-4306-96d1-1e0e15b0ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def make_group_stratified_folds(\n",
    "    df_all: pd.DataFrame,\n",
    "    X_all: np.ndarray,\n",
    "    n_splits: int = 5,\n",
    "    group_from: str = \"audio_path\",\n",
    "    class_col: str = \"class\",\n",
    "    month_col: str = \"month_folder\",\n",
    "    selected_classes: list | None = None,\n",
    "    selected_months: list | None = None,\n",
    "    shuffle: bool = True,\n",
    "    random_state: int | None = None,\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Leakage-safe, group-aware, stratified K-fold splits\n",
    "    with optional filtering and automatic label re-indexing.\n",
    "\n",
    "    IMPORTANT:\n",
    "    - Labels are re-indexed AFTER class/month selection\n",
    "      so they are contiguous: {0, 1, ..., K-1}\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(df_all) == len(X_all), \"df_all and X_all must be aligned\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 0. Optional filtering (NO leakage)\n",
    "    # --------------------------------------------------\n",
    "    mask = np.ones(len(df_all), dtype=bool)\n",
    "\n",
    "    if selected_classes is not None:\n",
    "        mask &= df_all[class_col].isin(selected_classes)\n",
    "\n",
    "    if selected_months is not None:\n",
    "        mask &= df_all[month_col].isin(selected_months)\n",
    "\n",
    "    df = df_all.loc[mask].copy().reset_index(drop=True)\n",
    "    X = X_all[mask]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nAfter filtering: {len(df)} samples\")\n",
    "        print(\"Class counts:\", df[class_col].value_counts().to_dict())\n",
    "        print(\"Month counts:\", df[month_col].value_counts().to_dict())\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Re-index labels AFTER selection (CRITICAL)\n",
    "    # --------------------------------------------------\n",
    "    unique_classes = sorted(df[class_col].unique())\n",
    "    class_to_label = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "    label_to_class = {i: cls for cls, i in class_to_label.items()}\n",
    "\n",
    "    df[\"label_id\"] = df[class_col].map(class_to_label)\n",
    "\n",
    "    num_classes = len(unique_classes)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nLabel re-indexing (used for training):\")\n",
    "        for cls, idx in class_to_label.items():\n",
    "            print(f\"  {cls} → {idx}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Extract group_id (leakage-safe)\n",
    "    # --------------------------------------------------\n",
    "    df[\"group_id\"] = (\n",
    "        df[group_from]\n",
    "        .str.split(\"/\")\n",
    "        .str[-1]\n",
    "        .str.split(\"_\")\n",
    "        .str[0]\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Group-level dataframe for stratification\n",
    "    # --------------------------------------------------\n",
    "    group_df = (\n",
    "        df.groupby(\"group_id\")\n",
    "        .agg({\n",
    "            class_col: \"first\",\n",
    "            month_col: \"first\"\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    group_df[\"strata\"] = (\n",
    "        group_df[class_col].astype(str) + \"_\" +\n",
    "        group_df[month_col].astype(str)\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Stratified K-fold on GROUPS\n",
    "    # --------------------------------------------------\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        shuffle=shuffle,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    folds = []\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 5. Build folds\n",
    "    # --------------------------------------------------\n",
    "    for fold_idx, (train_g_idx, test_g_idx) in enumerate(\n",
    "        skf.split(group_df[\"group_id\"], group_df[\"strata\"])\n",
    "    ):\n",
    "        train_group_ids = set(group_df.iloc[train_g_idx][\"group_id\"])\n",
    "        test_group_ids  = set(group_df.iloc[test_g_idx][\"group_id\"])\n",
    "\n",
    "        train_mask = df[\"group_id\"].isin(train_group_ids)\n",
    "        test_mask  = df[\"group_id\"].isin(test_group_ids)\n",
    "\n",
    "        X_train = X[train_mask.values]\n",
    "        X_test  = X[test_mask.values]\n",
    "\n",
    "        df_train = df.loc[train_mask].reset_index(drop=True)\n",
    "        df_test  = df.loc[test_mask].reset_index(drop=True)\n",
    "\n",
    "        # Sanity checks\n",
    "        assert train_group_ids.isdisjoint(test_group_ids)\n",
    "        assert len(X_train) + len(X_test) == len(X)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\nFold {fold_idx}: \"\n",
    "                f\"train={len(X_train)} ({len(X_train)/len(X):.2%}), \"\n",
    "                f\"test={len(X_test)} ({len(X_test)/len(X):.2%})\"\n",
    "            )\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": fold_idx,\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"df_train\": df_train,\n",
    "            \"df_test\": df_test,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"class_to_label\": class_to_label,\n",
    "            \"label_to_class\": label_to_class,\n",
    "            \"train_group_ids\": train_group_ids,\n",
    "            \"test_group_ids\": test_group_ids\n",
    "        })\n",
    "\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6659c74f-905b-4cbf-b118-3b1a8f55238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After filtering: 2190 samples\n",
      "Class counts: {'Hungry': 1000, 'Sleepy': 600, 'Wakeup': 590}\n",
      "Month counts: {'3Month': 750, '2Month': 700, '0Month': 400, '1Month': 340}\n",
      "\n",
      "Label re-indexing (used for training):\n",
      "  Hungry → 0\n",
      "  Sleepy → 1\n",
      "  Wakeup → 2\n",
      "\n",
      "Fold 0: train=2085 (95.21%), test=105 (4.79%)\n",
      "\n",
      "Fold 1: train=2078 (94.89%), test=112 (5.11%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uottawa.o.univ/njaza024/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=20.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2: train=2092 (95.53%), test=98 (4.47%)\n",
      "\n",
      "Fold 3: train=2080 (94.98%), test=110 (5.02%)\n",
      "\n",
      "Fold 4: train=2070 (94.52%), test=120 (5.48%)\n",
      "\n",
      "Fold 5: train=2052 (93.70%), test=138 (6.30%)\n",
      "\n",
      "Fold 6: train=2058 (93.97%), test=132 (6.03%)\n",
      "\n",
      "Fold 7: train=2040 (93.15%), test=150 (6.85%)\n",
      "\n",
      "Fold 8: train=2027 (92.56%), test=163 (7.44%)\n",
      "\n",
      "Fold 9: train=2080 (94.98%), test=110 (5.02%)\n",
      "\n",
      "Fold 10: train=2129 (97.21%), test=61 (2.79%)\n",
      "\n",
      "Fold 11: train=2092 (95.53%), test=98 (4.47%)\n",
      "\n",
      "Fold 12: train=2108 (96.26%), test=82 (3.74%)\n",
      "\n",
      "Fold 13: train=2038 (93.06%), test=152 (6.94%)\n",
      "\n",
      "Fold 14: train=2082 (95.07%), test=108 (4.93%)\n",
      "\n",
      "Fold 15: train=2066 (94.34%), test=124 (5.66%)\n",
      "\n",
      "Fold 16: train=2117 (96.67%), test=73 (3.33%)\n",
      "\n",
      "Fold 17: train=2119 (96.76%), test=71 (3.24%)\n",
      "\n",
      "Fold 18: train=2079 (94.93%), test=111 (5.07%)\n",
      "\n",
      "Fold 19: train=2118 (96.71%), test=72 (3.29%)\n"
     ]
    }
   ],
   "source": [
    "folds = make_group_stratified_folds(\n",
    "    df_all,\n",
    "    X_all,\n",
    "    n_splits=20,\n",
    "    selected_months=[\"0Month\", \"1Month\", \"2Month\", \"3Month\"],\n",
    "    selected_classes=[\"Sleepy\", \"Hungry\", \"Wakeup\"],\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0da9b12-640d-4065-bbcf-c72e6d5bd67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FOLD 0\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 250\n",
      "Samples per ID: min=1, max=45, mean=8.34\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Sleepy03MB00011 Sleepy       3Month           34\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 14\n",
      "Samples per ID: min=1, max=22, mean=7.50\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Sleepy03MB00008 Sleepy       3Month           22\n",
      "Hungry00MB00009 Hungry       0Month           10\n",
      "Wakeup02MU00017 Wakeup       2Month           10\n",
      "Hungry02MB00006 Hungry       2Month            9\n",
      "Hungry01MB00004 Hungry       1Month            7\n",
      "\n",
      "================================================================================\n",
      "FOLD 1\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 250\n",
      "Samples per ID: min=1, max=45, mean=8.31\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Sleepy03MB00011 Sleepy       3Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 14\n",
      "Samples per ID: min=1, max=26, mean=8.00\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Hungry01MB00009 Hungry       1Month           26\n",
      "Sleepy03MB00031 Sleepy       3Month           22\n",
      "Wakeup02MG00001 Wakeup       2Month           12\n",
      "Hungry00MG00002 Hungry       0Month           12\n",
      "Hungry03MB00019 Hungry       3Month           11\n",
      "\n",
      "================================================================================\n",
      "FOLD 2\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 250\n",
      "Samples per ID: min=1, max=45, mean=8.37\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 14\n",
      "Samples per ID: min=1, max=20, mean=7.00\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Hungry00MG00001 Hungry       0Month           20\n",
      "Hungry01MB00014 Hungry       1Month           20\n",
      "Hungry02MB00017 Hungry       2Month           17\n",
      "Hungry03MG00012 Hungry       3Month           10\n",
      "Hungry03MG00013 Hungry       3Month            8\n",
      "\n",
      "================================================================================\n",
      "FOLD 3\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 250\n",
      "Samples per ID: min=1, max=45, mean=8.32\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 14\n",
      "Samples per ID: min=2, max=20, mean=7.86\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Hungry03MG00007 Hungry       3Month           20\n",
      "Hungry03MG00009 Hungry       3Month           15\n",
      "Hungry03MB00034 Hungry       3Month           13\n",
      "Hungry01MB00007 Hungry       1Month           11\n",
      "Sleepy02MG00002 Sleepy       2Month            9\n",
      "\n",
      "================================================================================\n",
      "FOLD 4\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.25\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Sleepy03MB00011 Sleepy       3Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=21, mean=9.23\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Sleepy00MB00001 Sleepy       0Month           21\n",
      "Hungry01MB00010 Hungry       1Month           19\n",
      "Hungry00MG00005 Hungry       0Month           18\n",
      "Wakeup02MU00016 Wakeup       2Month           14\n",
      "Hungry02MB00015 Hungry       2Month           11\n",
      "\n",
      "================================================================================\n",
      "FOLD 5\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.18\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "Sleepy03MB00011 Sleepy       3Month           34\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=40, mean=10.62\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Hungry01MB00013 Hungry       1Month           29\n",
      "Hungry00MB00011 Hungry       0Month           17\n",
      "Hungry02MB00013 Hungry       2Month           13\n",
      "Sleepy02MB00023 Sleepy       2Month           11\n",
      "\n",
      "================================================================================\n",
      "FOLD 6\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.20\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=29, mean=10.15\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00004 Wakeup       0Month           29\n",
      "Sleepy00MB00002 Sleepy       0Month           26\n",
      "Hungry01MB00006 Hungry       1Month           18\n",
      "Hungry00MU00016 Hungry       0Month           17\n",
      "Hungry03MG00005 Hungry       3Month           16\n",
      "\n",
      "================================================================================\n",
      "FOLD 7\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=40, mean=8.13\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Sleepy03MB00011 Sleepy       3Month           34\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=45, mean=11.54\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Hungry00MG00006 Hungry       0Month           22\n",
      "Hungry03MG00006 Hungry       3Month           19\n",
      "Wakeup02MU00014 Wakeup       2Month           18\n",
      "Sleepy01MB00005 Sleepy       1Month           15\n",
      "\n",
      "================================================================================\n",
      "FOLD 8\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.08\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Wakeup03MB00004 Wakeup       3Month           32\n",
      "Hungry01MB00013 Hungry       1Month           29\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=36, mean=12.54\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Sleepy03MB00011 Sleepy       3Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "Sleepy01MB00003 Sleepy       1Month           17\n",
      "Hungry03MB00036 Hungry       3Month           10\n",
      "\n",
      "================================================================================\n",
      "FOLD 9\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.29\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Sleepy03MB00011 Sleepy       3Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=34, mean=8.46\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Wakeup01MB00001 Wakeup       1Month           18\n",
      "Sleepy02MB00020 Sleepy       2Month           15\n",
      "Hungry02MB00016 Hungry       2Month           15\n",
      "Hungry00MB00007 Hungry       0Month           12\n",
      "\n",
      "================================================================================\n",
      "FOLD 10\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.48\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=15, mean=4.69\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Hungry00MG00018 Hungry       0Month           15\n",
      "Wakeup02MU00012 Wakeup       2Month           10\n",
      "Hungry02MB00036 Hungry       2Month            9\n",
      "Sleepy02MB00021 Sleepy       2Month            6\n",
      "Hungry03MU00038 Hungry       3Month            6\n",
      "\n",
      "================================================================================\n",
      "FOLD 11\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.33\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=32, mean=7.54\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup03MB00004 Wakeup       3Month           32\n",
      "Sleepy02MG00004 Sleepy       2Month           15\n",
      "Sleepy01MU00009 Sleepy       1Month            9\n",
      "Hungry03MB00033 Hungry       3Month            9\n",
      "Wakeup01MB00003 Wakeup       1Month            8\n",
      "\n",
      "================================================================================\n",
      "FOLD 12\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.40\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=27, mean=6.31\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup03MG00002 Wakeup       3Month           27\n",
      "Hungry03MG00018 Hungry       3Month           12\n",
      "Hungry00MU00003 Hungry       0Month           12\n",
      "Hungry03MG00010 Hungry       3Month            8\n",
      "Hungry02MB00034 Hungry       2Month            7\n",
      "\n",
      "================================================================================\n",
      "FOLD 13\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.12\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Sleepy03MB00011 Sleepy       3Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=23, mean=11.69\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Hungry00MB00010 Hungry       0Month           23\n",
      "Wakeup03MB00042 Wakeup       3Month           22\n",
      "Wakeup03MB00005 Wakeup       3Month           21\n",
      "Sleepy03MB00007 Sleepy       3Month           19\n",
      "Hungry02MB00026 Hungry       2Month           16\n",
      "\n",
      "================================================================================\n",
      "FOLD 14\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.29\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=2, max=22, mean=8.31\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Sleepy01MG00001 Sleepy       1Month           22\n",
      "Wakeup02MG00002 Wakeup       2Month           22\n",
      "Hungry00MG00004 Hungry       0Month           12\n",
      "Sleepy02MG00001 Sleepy       2Month           11\n",
      "Hungry03MB00037 Hungry       3Month            8\n",
      "\n",
      "================================================================================\n",
      "FOLD 15\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.23\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=20, mean=9.54\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Sleepy01MB00006 Sleepy       1Month           20\n",
      "Hungry01MB00016 Hungry       1Month           19\n",
      "Sleepy03MB00030 Sleepy       3Month           17\n",
      "Hungry02MB00025 Hungry       2Month           13\n",
      "Hungry02MB00035 Hungry       2Month           11\n",
      "\n",
      "================================================================================\n",
      "FOLD 16\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.43\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=15, mean=5.62\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Sleepy03MG00001 Sleepy       3Month           15\n",
      "Hungry01MB00005 Hungry       1Month           11\n",
      "Sleepy02MB00015 Sleepy       2Month           10\n",
      "Hungry03MG00001 Hungry       3Month            9\n",
      "Wakeup03MB00040 Wakeup       3Month            7\n",
      "\n",
      "================================================================================\n",
      "FOLD 17\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.44\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=18, mean=5.46\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Hungry03MG00004 Hungry       3Month           18\n",
      "Sleepy03MB00009 Sleepy       3Month           16\n",
      "Hungry02MB00021 Hungry       2Month            8\n",
      "Hungry01MB00003 Hungry       1Month            6\n",
      "Hungry02MB00030 Hungry       2Month            4\n",
      "\n",
      "================================================================================\n",
      "FOLD 18\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.28\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Sleepy03MB00011 Sleepy       3Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=25, mean=8.54\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Hungry01MB00002 Hungry       1Month           25\n",
      "Wakeup03MG00001 Wakeup       3Month           24\n",
      "Wakeup02MU00015 Wakeup       2Month           14\n",
      "Sleepy03MB00034 Sleepy       3Month           12\n",
      "Hungry03MB00020 Hungry       3Month           11\n",
      "\n",
      "================================================================================\n",
      "FOLD 19\n",
      "================================================================================\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Number of unique IDs: 251\n",
      "Samples per ID: min=1, max=45, mean=8.44\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Wakeup00MG00003 Wakeup       0Month           45\n",
      "Wakeup00MG00005 Wakeup       0Month           40\n",
      "Wakeup00MG00002 Wakeup       0Month           36\n",
      "Wakeup02MB00005 Wakeup       2Month           34\n",
      "Wakeup02MB00010 Wakeup       2Month           34\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Number of unique IDs: 13\n",
      "Samples per ID: min=1, max=12, mean=5.54\n",
      "\n",
      "Top 5 IDs with most samples:\n",
      "       group_id  class month_folder  num_samples\n",
      "Sleepy02MG00003 Sleepy       2Month           12\n",
      "Hungry02MB00028 Hungry       2Month           10\n",
      "Hungry01MB00015 Hungry       1Month            9\n",
      "Hungry03MG00011 Hungry       3Month            8\n",
      "Sleepy02MB00019 Sleepy       2Month            7\n"
     ]
    }
   ],
   "source": [
    "def print_group_statistics(folds, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Print statistics of number of samples per (group_id, class, month)\n",
    "    for each fold, separately for train and test.\n",
    "    \"\"\"\n",
    "\n",
    "    for f in folds:\n",
    "        fold_id = f[\"fold\"]\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"FOLD {fold_id}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        for split_name, df_split in [\n",
    "            (\"TRAIN\", f[\"df_train\"]),\n",
    "            (\"TEST\",  f[\"df_test\"])\n",
    "        ]:\n",
    "            print(f\"\\n--- {split_name} SPLIT ---\")\n",
    "\n",
    "            # Count samples per (group_id, class, month)\n",
    "            counts = (\n",
    "                df_split\n",
    "                .groupby([\"group_id\", \"class\", \"month_folder\"])\n",
    "                .size()\n",
    "                .reset_index(name=\"num_samples\")\n",
    "            )\n",
    "\n",
    "            # Summary statistics\n",
    "            print(f\"Number of unique IDs: {counts['group_id'].nunique()}\")\n",
    "            print(\n",
    "                \"Samples per ID: \"\n",
    "                f\"min={counts['num_samples'].min()}, \"\n",
    "                f\"max={counts['num_samples'].max()}, \"\n",
    "                f\"mean={counts['num_samples'].mean():.2f}\"\n",
    "            )\n",
    "\n",
    "            # Show largest groups\n",
    "            print(f\"\\nTop {top_k} IDs with most samples:\")\n",
    "            print(\n",
    "                counts\n",
    "                .sort_values(\"num_samples\", ascending=False)\n",
    "                .head(top_k)\n",
    "                .to_string(index=False)\n",
    "            )\n",
    "\n",
    "\n",
    "print_group_statistics(folds, top_k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2b72f-bf46-43a5-89c0-cf02910eb146",
   "metadata": {},
   "source": [
    "# Baby2020 Fold0 Test and Train and y Train and y Yest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c89e899-b8b0-429d-b351-125aac0389e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold0 X_train: (2085, 100, 280) y_train: (2085,)\n",
      "Fold0 X_test : (105, 100, 280) y_test : (105,)\n",
      "class_to_label: {'Hungry': 0, 'Sleepy': 1, 'Wakeup': 2}\n",
      "label_to_class: {0: 'Hungry', 1: 'Sleepy', 2: 'Wakeup'}\n"
     ]
    }
   ],
   "source": [
    "# --- pick fold 0 ---\n",
    "fold0 = folds[0]   # or folds[fold_index]\n",
    "\n",
    "# --- features ---\n",
    "X_train0 = fold0[\"X_train\"]\n",
    "X_test0  = fold0[\"X_test\"]\n",
    "\n",
    "# --- labels (integer ids made in make_group_stratified_folds) ---\n",
    "y_train0 = fold0[\"df_train\"][\"label_id\"].to_numpy()\n",
    "y_test0  = fold0[\"df_test\"][\"label_id\"].to_numpy()\n",
    "\n",
    "print(\"Fold0 X_train:\", X_train0.shape, \"y_train:\", y_train0.shape)\n",
    "print(\"Fold0 X_test :\", X_test0.shape,  \"y_test :\", y_test0.shape)\n",
    "\n",
    "# --- optional: see class mapping for this fold setup ---\n",
    "print(\"class_to_label:\", fold0[\"class_to_label\"])\n",
    "print(\"label_to_class:\", fold0[\"label_to_class\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d853a107-560b-4ee6-ae1f-d191a4da085a",
   "metadata": {},
   "source": [
    "# Chinese Babycry Test and Train and y Train and y Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "126bf096-ff23-43ed-8fa4-9e8b28c7a8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese BabyCry 3-Mood Mapping:\n",
      "  0 → Diaper\n",
      "  1 → Sleepy\n",
      "  2 → Uncomfortable\n",
      "\n",
      "Shapes:\n",
      "  X_train_Chinese_babyCry: (350, 100, 280, 1)\n",
      "  y_train_Chinese_babyCry: (350,)\n",
      "  X_val_Chinese_babyCry  : (88, 100, 280, 1)\n",
      "  y_val_Chinese_babyCry  : (88,)\n",
      "\n",
      "Unique encoded train labels: [0 1 2]\n",
      "Unique encoded val labels  : [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ======================================================\n",
    "# Chinese BabyCry: select 3 moods\n",
    "# ======================================================\n",
    "\n",
    "# Original class IDs to keep\n",
    "Chinese_babyCry_keep_classes = [1, 4, 5]   # diaper=1, sleepy=4, uncomfortable=5\n",
    "\n",
    "Chinese_babyCry_class_names = {\n",
    "    1: \"Diaper\",\n",
    "    4: \"Sleepy\",\n",
    "    5: \"Uncomfortable\"\n",
    "}\n",
    "\n",
    "# ======================================================\n",
    "# Train split (Chinese BabyCry)\n",
    "# ======================================================\n",
    "Chinese_babyCry_mask_train = np.isin(y_train_chinese, Chinese_babyCry_keep_classes)\n",
    "\n",
    "X_train_Chinese_babyCry = X_train_chinese[Chinese_babyCry_mask_train]\n",
    "y_train_Chinese_babyCry = y_train_chinese[Chinese_babyCry_mask_train]\n",
    "\n",
    "# ======================================================\n",
    "# Test / Validation split (Chinese BabyCry)\n",
    "# ======================================================\n",
    "Chinese_babyCry_mask_test = np.isin(y_test_chinese, Chinese_babyCry_keep_classes)\n",
    "\n",
    "X_val_Chinese_babyCry = X_test_chinese[Chinese_babyCry_mask_test]\n",
    "y_val_Chinese_babyCry = y_test_chinese[Chinese_babyCry_mask_test]\n",
    "\n",
    "# ======================================================\n",
    "# Re-encode labels to {0,1,2}\n",
    "# ======================================================\n",
    "Chinese_babyCry_unique_classes = sorted(Chinese_babyCry_keep_classes)  # [1,4,5]\n",
    "Chinese_babyCry_class2newid = {\n",
    "    old: new for new, old in enumerate(Chinese_babyCry_unique_classes)\n",
    "}\n",
    "\n",
    "Chinese_babyCry_id2label = {\n",
    "    new: Chinese_babyCry_class_names[old]\n",
    "    for old, new in Chinese_babyCry_class2newid.items()\n",
    "}\n",
    "\n",
    "y_train_Chinese_babyCry = np.array(\n",
    "    [Chinese_babyCry_class2newid[y] for y in y_train_Chinese_babyCry],\n",
    "    dtype=np.int64\n",
    ")\n",
    "\n",
    "y_val_Chinese_babyCry = np.array(\n",
    "    [Chinese_babyCry_class2newid[y] for y in y_val_Chinese_babyCry],\n",
    "    dtype=np.int64\n",
    ")\n",
    "\n",
    "# ======================================================\n",
    "# Add channel dimension (CNN expects [..., 1])\n",
    "# ======================================================\n",
    "X_train_Chinese_babyCry = np.expand_dims(X_train_Chinese_babyCry, axis=-1)\n",
    "X_val_Chinese_babyCry   = np.expand_dims(X_val_Chinese_babyCry, axis=-1)\n",
    "\n",
    "# ======================================================\n",
    "# Sanity checks\n",
    "# ======================================================\n",
    "print(\"Chinese BabyCry 3-Mood Mapping:\")\n",
    "for k, v in Chinese_babyCry_id2label.items():\n",
    "    print(f\"  {k} → {v}\")\n",
    "\n",
    "print(\"\\nShapes:\")\n",
    "print(\"  X_train_Chinese_babyCry:\", X_train_Chinese_babyCry.shape)\n",
    "print(\"  y_train_Chinese_babyCry:\", y_train_Chinese_babyCry.shape)\n",
    "print(\"  X_val_Chinese_babyCry  :\", X_val_Chinese_babyCry.shape)\n",
    "print(\"  y_val_Chinese_babyCry  :\", y_val_Chinese_babyCry.shape)\n",
    "\n",
    "print(\"\\nUnique encoded train labels:\", np.unique(y_train_Chinese_babyCry))\n",
    "print(\"Unique encoded val labels  :\", np.unique(y_val_Chinese_babyCry))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e81ad-8b48-4075-bebc-2cfcf0189b48",
   "metadata": {},
   "source": [
    "# Re evaluate Loaded models  accuracy and F1 score on test baby2020 and Chinese baby cry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24a180f2-8e42-4bee-abdb-b719a5949013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "def evaluate_keras_model(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    label_names,\n",
    "    name=\"Model\",\n",
    "    batch_size=32\n",
    "):\n",
    "    \"\"\"\n",
    "    model: tf.keras.Model (softmax output is fine)\n",
    "    X_test: np array, shape [N, T, F, 1] (or whatever model expects)\n",
    "    y_test: int labels [N] with values in {0..C-1}\n",
    "    label_names: list of class names in EXACT output order of model\n",
    "    \"\"\"\n",
    "    # ---------- sanity checks ----------\n",
    "    C = model.output_shape[-1]\n",
    "    assert C == len(label_names), f\"{name}: output classes={C} but label_names={len(label_names)}\"\n",
    "    assert X_test.shape[0] == len(y_test), f\"{name}: X_test and y_test length mismatch\"\n",
    "    assert np.min(y_test) >= 0 and np.max(y_test) < C, \\\n",
    "        f\"{name}: y_test has labels outside [0,{C-1}]. min={np.min(y_test)}, max={np.max(y_test)}\"\n",
    "\n",
    "    # ---------- predict ----------\n",
    "    probs = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "\n",
    "    # ---------- metrics ----------\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"✅ {name} — Test Results\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Accuracy     : {acc:.4f}\")\n",
    "    print(f\"F1 (macro)   : {f1_macro:.4f}\")\n",
    "    print(f\"F1 (weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"MCC          : {mcc:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_names, digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(cm)\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"mcc\": mcc,\n",
    "        \"cm\": cm,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"probs\": probs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3d7d78-8b4d-409d-a12f-d84e44546b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1767555659.015342 1347469 cuda_dnn.cc:529] Loaded cuDNN version 90800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✅ Baby2020 — Test Results\n",
      "================================================================================\n",
      "Accuracy     : 0.9669\n",
      "F1 (macro)   : 0.9661\n",
      "F1 (weighted): 0.9670\n",
      "MCC          : 0.9491\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Sleepy     0.9756    0.9664    0.9710       952\n",
      "      Hungry     0.9963    0.9450    0.9700       564\n",
      "      Wakeup     0.9275    0.9895    0.9575       569\n",
      "\n",
      "    accuracy                         0.9669      2085\n",
      "   macro avg     0.9665    0.9670    0.9661      2085\n",
      "weighted avg     0.9681    0.9669    0.9670      2085\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[920   1  31]\n",
      " [ 18 533  13]\n",
      " [  5   1 563]]\n"
     ]
    }
   ],
   "source": [
    "# Example variable names — replace with yours:\n",
    "# X_test_Baby2020 = X_val_Baby2020 or X_test_split_Baby2020\n",
    "# y_test_Baby2020 = y_val_Baby2020 or y_test_split_Baby2020\n",
    "\n",
    "baby_results = evaluate_keras_model(\n",
    "    baby_model,\n",
    "    X_train0,\n",
    "    y_train0,\n",
    "    label_names=BABY2020_LABELS,\n",
    "    name=\"Baby2020\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b42438f0-0be2-4075-885f-fb508dd899e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✅ Baby2020 — Test Results\n",
      "================================================================================\n",
      "Accuracy     : 0.6857\n",
      "F1 (macro)   : 0.6540\n",
      "F1 (weighted): 0.6767\n",
      "MCC          : 0.5311\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Sleepy     0.7843    0.8333    0.8081        48\n",
      "      Hungry     0.7500    0.4167    0.5357        36\n",
      "      Wakeup     0.5000    0.8095    0.6182        21\n",
      "\n",
      "    accuracy                         0.6857       105\n",
      "   macro avg     0.6781    0.6865    0.6540       105\n",
      "weighted avg     0.7157    0.6857    0.6767       105\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[40  3  5]\n",
      " [ 9 15 12]\n",
      " [ 2  2 17]]\n"
     ]
    }
   ],
   "source": [
    "# Example variable names — replace with yours:\n",
    "# X_test_Baby2020 = X_val_Baby2020 or X_test_split_Baby2020\n",
    "# y_test_Baby2020 = y_val_Baby2020 or y_test_split_Baby2020\n",
    "\n",
    "baby_results = evaluate_keras_model(\n",
    "    baby_model,\n",
    "    X_test0,\n",
    "    y_test0,\n",
    "    label_names=BABY2020_LABELS,\n",
    "    name=\"Baby2020\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a84608da-b56d-46aa-b402-59e84fa66467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✅ Chinese BabyCry — Test Results\n",
      "================================================================================\n",
      "Accuracy     : 0.9971\n",
      "F1 (macro)   : 0.9972\n",
      "F1 (weighted): 0.9971\n",
      "MCC          : 0.9957\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Diaper     1.0000    1.0000    1.0000       107\n",
      "Uncomfortable     0.9914    1.0000    0.9957       115\n",
      "       Sleepy     1.0000    0.9922    0.9961       128\n",
      "\n",
      "     accuracy                         0.9971       350\n",
      "    macro avg     0.9971    0.9974    0.9972       350\n",
      " weighted avg     0.9972    0.9971    0.9971       350\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[107   0   0]\n",
      " [  0 115   0]\n",
      " [  0   1 127]]\n"
     ]
    }
   ],
   "source": [
    "china_results = evaluate_keras_model(\n",
    "    china_model,\n",
    "    X_train_Chinese_babyCry,\n",
    "    y_train_Chinese_babyCry,\n",
    "    label_names=CHINESE_LABELS,\n",
    "    name=\"Chinese BabyCry\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f6c754a-183e-4e08-8ead-2536c4723794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✅ Chinese BabyCry — Test Results\n",
      "================================================================================\n",
      "Accuracy     : 0.7273\n",
      "F1 (macro)   : 0.7251\n",
      "F1 (weighted): 0.7256\n",
      "MCC          : 0.5928\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Diaper     0.7000    0.7778    0.7368        27\n",
      "Uncomfortable     0.7826    0.6207    0.6923        29\n",
      "       Sleepy     0.7143    0.7812    0.7463        32\n",
      "\n",
      "     accuracy                         0.7273        88\n",
      "    macro avg     0.7323    0.7266    0.7251        88\n",
      " weighted avg     0.7324    0.7273    0.7256        88\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[21  2  4]\n",
      " [ 5 18  6]\n",
      " [ 4  3 25]]\n"
     ]
    }
   ],
   "source": [
    "china_results = evaluate_keras_model(\n",
    "    china_model,\n",
    "    X_val_Chinese_babyCry,\n",
    "    y_val_Chinese_babyCry,\n",
    "    label_names=CHINESE_LABELS,\n",
    "    name=\"Chinese BabyCry\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ca2c7ee-bfc7-4a5f-aa74-1feed81a43da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_Chinese_babyCry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331d3c3-dffd-4635-94c4-6d09695599a1",
   "metadata": {},
   "source": [
    "# Test Baby2020 model on Chinese dataset and vise versa before Ensemble mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4dd09e0-5bf1-485c-b470-33c96f8e3f76",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "as_list() is not defined on an unknown TensorShape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1347034/180583780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m china_results = evaluate_keras_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mchina_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX_test0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my_test0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabel_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCHINESE_LABELS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1347034/3504000793.py\u001b[0m in \u001b[0;36mevaluate_keras_model\u001b[0;34m(model, X_test, y_test, label_names, name, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# ---------- predict ----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."
     ]
    }
   ],
   "source": [
    "china_results = evaluate_keras_model(\n",
    "    china_model,\n",
    "    X_test0,\n",
    "    y_test0,\n",
    "    label_names=CHINESE_LABELS,\n",
    "    name=\"Chinese BabyCry\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e0517-8347-45cc-b073-51c07fbe86a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_results = evaluate_keras_model(\n",
    "    baby_model,\n",
    "    X_train_Chinese_babyCry ,\n",
    "    y_train_Chinese_babyCry ,\n",
    "    label_names=BABY2020_LABELS,\n",
    "    name=\"Baby2020\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da76f6-4bc5-4a4c-8706-c0dbfb04be30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb66613-24c3-4efa-aa4f-2c3b563fc996",
   "metadata": {},
   "source": [
    "# Temperature scaling (optimize scalar T)\n",
    "\n",
    "### This learns one scalar T per model using its validation set (D_m^val)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c4c32-9152-4771-9d56-b92b91906ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "# def collect_logits_and_labels(model, loader, device):\n",
    "#     model.eval()\n",
    "#     all_logits, all_y = [], []\n",
    "#     for xb, yb in loader:\n",
    "#         xb = xb.to(device)\n",
    "#         yb = yb.to(device)\n",
    "#         logits = model(xb)\n",
    "#         all_logits.append(logits.detach().cpu())\n",
    "#         all_y.append(yb.detach().cpu())\n",
    "#     return torch.cat(all_logits, dim=0), torch.cat(all_y, dim=0)\n",
    "\n",
    "# class TemperatureScaler(nn.Module):\n",
    "#     def __init__(self, init_T=1.0):\n",
    "#         super().__init__()\n",
    "#         # optimize logT for positivity: T = exp(logT)\n",
    "#         self.logT = nn.Parameter(torch.tensor([math.log(init_T)], dtype=torch.float32))\n",
    "\n",
    "#     def forward(self, logits):\n",
    "#         T = torch.exp(self.logT)\n",
    "#         return logits / T\n",
    "\n",
    "# def fit_temperature(logits, labels, max_iter=2000, lr=0.05, verbose=True):\n",
    "#     \"\"\"\n",
    "#     logits: [N, C] (CPU tensor)\n",
    "#     labels: [N]    (CPU tensor) with class indices in that model's label space\n",
    "#     \"\"\"\n",
    "#     scaler = TemperatureScaler(init_T=1.0)\n",
    "#     optimizer = torch.optim.LBFGS(scaler.parameters(), lr=lr, max_iter=max_iter)\n",
    "#     nll = nn.CrossEntropyLoss()\n",
    "\n",
    "#     logits = logits.float()\n",
    "#     labels = labels.long()\n",
    "\n",
    "#     def closure():\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = nll(scaler(logits), labels)\n",
    "#         loss.backward()\n",
    "#         return loss\n",
    "\n",
    "#     loss = optimizer.step(closure)\n",
    "#     T = torch.exp(scaler.logT).item()\n",
    "#     if verbose:\n",
    "#         print(f\"✅ Learned temperature T = {T:.4f} | NLL = {loss.item():.4f}\")\n",
    "#     return T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e704c-1347-4a47-b50c-ba42d9333149",
   "metadata": {},
   "source": [
    "# Fit T_B and T_C from validation loaders\n",
    "\n",
    "### I must provide two loaders:\n",
    "\n",
    "### baby_val_loader (Baby2020 val set)\n",
    "\n",
    "### china_val_loader (Chinese val set)\n",
    "\n",
    "## They should yield (xb, yb) where yb is in the model’s own label indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8bac93-d113-42b5-ae1f-19801ab1fbe9",
   "metadata": {},
   "source": [
    "## Create a simple Dataset wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c8fbc-5938-43df-b017-473d715e6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc12bb-4e37-449c-b8de-f47299c82b82",
   "metadata": {},
   "source": [
    "## Build calibration loaders (THIS is what you asked for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ad58b-9f46-4c3f-b622-247cd2bfad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Chinese calibration loader\n",
    "# ---------------------------\n",
    "china_val_dataset = NumpyDataset(\n",
    "    X_val_Chinese_babyCry,\n",
    "    y_val_Chinese_babyCry\n",
    ")\n",
    "\n",
    "china_val_loader = DataLoader(\n",
    "    china_val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False   # IMPORTANT: never shuffle calibration data\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Baby calibration loader\n",
    "# ---------------------------\n",
    "baby_val_dataset = NumpyDataset(\n",
    "    X_test0,\n",
    "    y_test0\n",
    ")\n",
    "\n",
    "baby_val_loader = DataLoader(\n",
    "    baby_val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe3f86-00c5-43f5-99ac-b1dccbc5f5ae",
   "metadata": {},
   "source": [
    "# Collect logits + labels from a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ec113-0370-4d9e-8a39-bc78b3bcbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def collect_logits_and_labels_keras(logits_model, X, y, batch_size=32):\n",
    "    \"\"\"\n",
    "    logits_model: Keras model that outputs logits (pre-softmax), shape [N,C]\n",
    "    X: numpy array\n",
    "    y: numpy int labels, shape [N]\n",
    "    \"\"\"\n",
    "    y = np.asarray(y).astype(np.int64)\n",
    "    logits = logits_model.predict(X, batch_size=batch_size, verbose=0)\n",
    "    return logits.astype(np.float32), y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaadd639-0057-4e9e-b585-12891d11e84b",
   "metadata": {},
   "source": [
    "# Fit temperature in TensorFlow (minimize NLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1155200-4f48-428d-b3ae-2bff48db14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_temperature_keras(logits, y, max_iter=200, lr=0.01, verbose=True):\n",
    "    \"\"\"\n",
    "    logits: numpy [N,C] (pre-softmax)\n",
    "    y: numpy [N] int labels in [0..C-1]\n",
    "    returns: scalar temperature T > 0\n",
    "    \"\"\"\n",
    "    logits_tf = tf.convert_to_tensor(logits, dtype=tf.float32)\n",
    "    y_tf = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "\n",
    "    # optimize log_T so T = exp(log_T) > 0 always\n",
    "    log_T = tf.Variable(0.0, dtype=tf.float32)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    best = np.inf\n",
    "    for i in range(max_iter):\n",
    "        with tf.GradientTape() as tape:\n",
    "            T = tf.exp(log_T)\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=y_tf,\n",
    "                    logits=logits_tf / T\n",
    "                )\n",
    "            )\n",
    "        grads = tape.gradient(loss, [log_T])\n",
    "        opt.apply_gradients(zip(grads, [log_T]))\n",
    "\n",
    "        if float(loss.numpy()) < best:\n",
    "            best = float(loss.numpy())\n",
    "\n",
    "        if verbose and (i % 25 == 0 or i == max_iter - 1):\n",
    "            print(f\"iter {i:03d} | NLL={loss.numpy():.5f} | T={tf.exp(log_T).numpy():.5f}\")\n",
    "\n",
    "    return float(tf.exp(log_T).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63684c9a-e47e-4bee-9940-a57c74e985ba",
   "metadata": {},
   "source": [
    "# Use YOUR calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68722ac3-b44c-4ba4-9c75-c28367fbbf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_4d_np(X):\n",
    "    X = np.asarray(X)\n",
    "    if X.ndim == 3:\n",
    "        X = np.expand_dims(X, axis=-1)\n",
    "    return X\n",
    "\n",
    "# Ensure shapes\n",
    "X_val_Chinese_babyCry = ensure_4d_np(X_val_Chinese_babyCry)\n",
    "X_test0               = ensure_4d_np(X_test0)\n",
    "\n",
    "# Collect logits (use logits models!)\n",
    "baby_logits, baby_y = collect_logits_and_labels_keras(\n",
    "    baby_logits_model,   # <-- IMPORTANT: logits wrapper\n",
    "    X_test0,\n",
    "    y_test0,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "china_logits, china_y = collect_logits_and_labels_keras(\n",
    "    china_logits_model,  # <-- IMPORTANT: logits wrapper\n",
    "    X_val_Chinese_babyCry,\n",
    "    y_val_Chinese_babyCry,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"Baby logits:\", baby_logits.shape, \"labels:\", baby_y.shape)\n",
    "print(\"China logits:\", china_logits.shape, \"labels:\", china_y.shape)\n",
    "\n",
    "# Fit temperatures\n",
    "T_B = fit_temperature_keras(baby_logits, baby_y, max_iter=200, lr=0.01, verbose=True)\n",
    "T_C = fit_temperature_keras(china_logits, china_y, max_iter=200, lr=0.01, verbose=True)\n",
    "\n",
    "print(f\"\\n✅ Temperature Baby2020: T_B = {T_B:.4f}\")\n",
    "print(f\"✅ Temperature Chinese : T_C = {T_C:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fbee20-9159-4b92-95bb-9b54839e4e69",
   "metadata": {},
   "source": [
    "# Safety Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a6f96-6f6c-4393-bbaf-2df3e4ebc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unique baby_y :\", np.unique(baby_y), \"C_baby:\", baby_logits.shape[1])\n",
    "print(\"unique china_y:\", np.unique(china_y), \"C_china:\", china_logits.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6e3e1-de5d-4b16-bb1b-4559c53aff4b",
   "metadata": {},
   "source": [
    "# Define label spaces + mappings (union logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac68ab-9b9b-449c-9a33-b29144a54c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "# ----------------------------\n",
    "# UNION LABEL SPACE (final output)\n",
    "# ----------------------------\n",
    "UNION_LABELS = [\"Diaper\", \"Uncomfortable\", \"Sleepy\", \"Hungry\", \"Wakeup\"]\n",
    "U = len(UNION_LABELS)\n",
    "union_idx = {name:i for i,name in enumerate(UNION_LABELS)}\n",
    "\n",
    "# ----------------------------\n",
    "# BABY model output mapping (3-class) -> union indices\n",
    "# Update these if your Baby2020 labels are different\n",
    "# Example: [\"Sleepy\", \"Hungry\", \"Wakeup\"]  (you had this earlier)\n",
    "# ----------------------------\n",
    "BABY_LABELS_3 = [\"Sleepy\", \"Hungry\", \"Wakeup\"]\n",
    "baby_to_union = np.array([union_idx[c] for c in BABY_LABELS_3], dtype=int)  # length 3\n",
    "\n",
    "# ----------------------------\n",
    "# CHINESE model 6-class outputs (we keep 1,4,5) -> union indices\n",
    "# 6-class index meaning (your assumption)\n",
    "CHINESE_LABELS_6 = [\"Hungry\",\"Diaper\",\"Pain\",\"Awake\",\"Sleepy\",\"Uncomfortable\"]\n",
    "keep_idx_6 = [1,4,5]  # Diaper, Sleepy, Uncomfortable\n",
    "CHINESE_KEEP_NAMES = [\"Diaper\",\"Sleepy\",\"Uncomfortable\"]\n",
    "china_keep_to_union = np.array([union_idx[c] for c in CHINESE_KEEP_NAMES], dtype=int)  # length 3\n",
    "\n",
    "print(\"baby_to_union:\", baby_to_union, \"=>\", [UNION_LABELS[i] for i in baby_to_union])\n",
    "print(\"china_keep_to_union:\", china_keep_to_union, \"=>\", [UNION_LABELS[i] for i in china_keep_to_union])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a492ea5f-ad91-4983-bc4f-418ce9249e74",
   "metadata": {},
   "source": [
    "# Helpers: logits → calibrated probs, entropy, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a6ba3-fd27-4e02-a547-61f2eee49b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_temp(logits, T=1.0):\n",
    "    z = logits / float(T)\n",
    "    z = z - np.max(z, axis=1, keepdims=True)  # stability\n",
    "    e = np.exp(z)\n",
    "    return e / (np.sum(e, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def entropy(p):\n",
    "    # p: [N,C]\n",
    "    return -np.sum(p * np.log(p + 1e-12), axis=1)  # [N]\n",
    "\n",
    "def entropy_weights(p_baby, p_china, tau=1.0):\n",
    "    Hb = entropy(p_baby)\n",
    "    Hc = entropy(p_china)\n",
    "    wb = np.exp(-tau * Hb)\n",
    "    wc = np.exp(-tau * Hc)\n",
    "    s = wb + wc + 1e-12\n",
    "    return wb/s, wc/s  # each [N]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2479465a-1ff6-40a0-973a-94ba4393ad7d",
   "metadata": {},
   "source": [
    "# Fusion function (exactly as algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c4099b-8af3-457e-b0b0-b9c16ad644da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_batch_union_logits(\n",
    "    baby_logits_3,   # [N,3] logits (pre-softmax)\n",
    "    china_logits_6,  # [N,6] logits (pre-softmax)\n",
    "    T_B, T_C,\n",
    "    baby_to_union,\n",
    "    keep_idx_6,\n",
    "    china_keep_to_union,\n",
    "    tau=1.0,\n",
    "):\n",
    "    N = baby_logits_3.shape[0]\n",
    "\n",
    "    # ---- calibrated posteriors for entropy weighting ----\n",
    "    p_b = softmax_temp(baby_logits_3, T_B)           # [N,3]\n",
    "    p_c6 = softmax_temp(china_logits_6, T_C)         # [N,6]\n",
    "    p_c = p_c6[:, keep_idx_6]                        # [N,3] (Diaper/Sleepy/Uncomfortable)\n",
    "    p_c = p_c / (np.sum(p_c, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    # ---- entropy-gated weights per sample ----\n",
    "    w_b, w_c = entropy_weights(p_b, p_c, tau=tau)    # each [N]\n",
    "\n",
    "    # ---- initialize union logits ----\n",
    "    z_union = np.full((N, U), -1e30, dtype=np.float32)  # -inf approx\n",
    "\n",
    "    # ---- fill Baby logits into union ----\n",
    "    for k in range(3):\n",
    "        z_union[:, baby_to_union[k]] = baby_logits_3[:, k] / float(T_B)\n",
    "\n",
    "    # ---- fill Chinese kept logits into union (but DON'T finalize sleepy yet) ----\n",
    "    # Chinese kept order: [Diaper, Sleepy, Uncomfortable]\n",
    "    # Map them to union indices [Diaper, Sleepy, Uncomfortable]\n",
    "    # We'll temporarily write them; then overwrite sleepy with fused version.\n",
    "    china_logits_kept = (china_logits_6[:, keep_idx_6] / float(T_C))  # [N,3]\n",
    "    for k in range(3):\n",
    "        z_union[:, china_keep_to_union[k]] = china_logits_kept[:, k]\n",
    "\n",
    "    # ---- fuse shared label: Sleepy ----\n",
    "    sleepy_u = union_idx[\"Sleepy\"]\n",
    "\n",
    "    # calibrated sleepy logits from each model\n",
    "    # Baby sleepy is where in BABY_LABELS_3? (usually index 0 if [\"Sleepy\",\"Hungry\",\"Wakeup\"])\n",
    "    baby_sleepy_k = BABY_LABELS_3.index(\"Sleepy\")\n",
    "    zB = baby_logits_3[:, baby_sleepy_k] / float(T_B)\n",
    "\n",
    "    # Chinese sleepy is 6-class index 4\n",
    "    zC = china_logits_6[:, 4] / float(T_C)\n",
    "\n",
    "    # log( wC*exp(zC) + wB*exp(zB) )\n",
    "    z_union[:, sleepy_u] = np.log(w_c * np.exp(zC) + w_b * np.exp(zB) + 1e-12)\n",
    "\n",
    "    # ---- final posterior over UNION ----\n",
    "    # softmax union logits\n",
    "    z = z_union - np.max(z_union, axis=1, keepdims=True)\n",
    "    p_union = np.exp(z) / (np.sum(np.exp(z), axis=1, keepdims=True) + 1e-12)  # [N,5]\n",
    "\n",
    "    y_pred_union = np.argmax(p_union, axis=1)\n",
    "\n",
    "    return p_union, y_pred_union, (w_b, w_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16650c28-51e1-44e1-a2ee-5b471fe12849",
   "metadata": {},
   "source": [
    "# Run ensemble on a dataset (Baby test OR Chinese test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bef9a-2d83-4dd4-8706-22bf698fb834",
   "metadata": {},
   "source": [
    "Baby test truth mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b338a9ee-a0d5-44b2-b7bf-d72bcc938289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_baby_y_to_union(y_baby_012):\n",
    "    # y_baby_012: {0,1,2} indexing BABY_LABELS_3\n",
    "    return np.array([union_idx[BABY_LABELS_3[i]] for i in y_baby_012], dtype=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083ca54-2349-44ac-999a-53b0965b151a",
   "metadata": {},
   "source": [
    "Chinese test truth mapping (3-of-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feafd84-5999-4b23-8299-35687c7b2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_china3_y_to_union(y_china_012):\n",
    "    # 0->Diaper, 1->Sleepy, 2->Uncomfortable\n",
    "    names = [\"Diaper\",\"Sleepy\",\"Uncomfortable\"]\n",
    "    return np.array([union_idx[names[i]] for i in y_china_012], dtype=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002dd55-25bc-4837-a238-fe2b5d37b4b6",
   "metadata": {},
   "source": [
    "# Collect logits for BOTH models on the SAME inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a62e5d-d590-4544-b22c-9c0fd26f80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits_keras(logits_model, X, batch_size=32):\n",
    "    return logits_model.predict(X, batch_size=batch_size, verbose=0).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b392208-e3cd-4106-8766-cf25b3cc33e8",
   "metadata": {},
   "source": [
    "# Evaluate on Baby test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512c141-dd03-49b9-9c5b-26bf3f181c53",
   "metadata": {},
   "source": [
    "# Evaluate on Chinese test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c3161-74e3-4195-81a5-3843cfffb589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ----------------------------\n",
    "# Label spaces\n",
    "# ----------------------------\n",
    "UNION_LABELS  = [\"Diaper\", \"Uncomfortable\", \"Sleepy\", \"Hungry\", \"Wakeup\"]\n",
    "BABY_LABELS_3 = [\"Hungry\", \"Sleepy\",  \"Wakeup\"]\n",
    "CHINA_LABELS_3 = [\"Diaper\", \"Sleepy\", \"Uncomfortable\"]  # <-- your choice B ✅\n",
    "\n",
    "U = len(UNION_LABELS)\n",
    "union_idx = {n:i for i,n in enumerate(UNION_LABELS)}\n",
    "\n",
    "def softmax_temp(logits, T=1.0):\n",
    "    z = logits / float(T)\n",
    "    z = z - np.max(z, axis=1, keepdims=True)  # stability\n",
    "    e = np.exp(z)\n",
    "    return e / (np.sum(e, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def entropy(p):\n",
    "    return -np.sum(p * np.log(p + 1e-12), axis=1)\n",
    "\n",
    "def entropy_weights(p_b, p_c, tau=1.0):\n",
    "    Hb = entropy(p_b)\n",
    "    Hc = entropy(p_c)\n",
    "    wb = np.exp(-tau * Hb)\n",
    "    wc = np.exp(-tau * Hc)\n",
    "    s = wb + wc + 1e-12\n",
    "    return wb/s, wc/s\n",
    "\n",
    "def get_logits_keras(logits_model, X, batch_size=32):\n",
    "    return logits_model.predict(X, batch_size=batch_size, verbose=0).astype(np.float32)\n",
    "\n",
    "\n",
    "def fuse_union_3v3(\n",
    "    baby_logits_3,   # [N,3] (Sleepy, Hungry, Wakeup)\n",
    "    china_logits_3,  # [N,3] (Diaper, Sleepy, Uncomfortable)\n",
    "    T_B, T_C,\n",
    "    tau=1.0\n",
    "):\n",
    "    N = baby_logits_3.shape[0]\n",
    "    z_union = np.full((N, U), -1e30, dtype=np.float32)  # -inf approx\n",
    "\n",
    "    # 1) Calibrated probs for entropy weights\n",
    "    p_b = softmax_temp(baby_logits_3, T_B)   # [N,3]\n",
    "    p_c = softmax_temp(china_logits_3, T_C)  # [N,3]\n",
    "\n",
    "    w_b, w_c = entropy_weights(p_b, p_c, tau=tau)  # each [N]\n",
    "\n",
    "    Hb = entropy(p_b)\n",
    "    Hc = entropy(p_c)\n",
    "    \n",
    "    print(\"Mean entropy baby :\", Hb.mean())\n",
    "    print(\"Mean entropy china:\", Hc.mean())\n",
    "    print(\"Mean w_b:\", w_b.mean(), \"Mean w_c:\", w_c.mean())\n",
    "    print(\"Frac w_c > 0.5:\", np.mean(w_c > 0.5))\n",
    "\n",
    "\n",
    "    # 2) Calibrated logits\n",
    "    zB = baby_logits_3 / float(T_B)   # [N,3]\n",
    "    zC = china_logits_3 / float(T_C)  # [N,3]\n",
    "\n",
    "    # 3) Fill disjoint labels directly into union logits\n",
    "    # Baby: Sleepy, Hungry, Wakeup\n",
    "    for k, name in enumerate(BABY_LABELS_3):\n",
    "        z_union[:, union_idx[name]] = zB[:, k]\n",
    "\n",
    "    # China: Diaper, Sleepy, Uncomfortable\n",
    "    for k, name in enumerate(CHINA_LABELS_3):\n",
    "        z_union[:, union_idx[name]] = zC[:, k]\n",
    "\n",
    "    # 4) Fuse shared class Sleepy with weighted log-sum-exp\n",
    "    sleepy_u = union_idx[\"Sleepy\"]\n",
    "    zB_sleepy = zB[:, BABY_LABELS_3.index(\"Sleepy\")]      # baby sleepy logit\n",
    "    zC_sleepy = zC[:, CHINA_LABELS_3.index(\"Sleepy\")]     # china sleepy logit\n",
    "\n",
    "    z_union[:, sleepy_u] = np.log(w_c * np.exp(zC_sleepy) + w_b * np.exp(zB_sleepy) + 1e-12)\n",
    "\n",
    "    # 5) Final softmax over union\n",
    "    z = z_union - np.max(z_union, axis=1, keepdims=True)\n",
    "    p_union = np.exp(z) / (np.sum(np.exp(z), axis=1, keepdims=True) + 1e-12)\n",
    "    y_pred_union = np.argmax(p_union, axis=1)\n",
    "\n",
    "    return p_union, y_pred_union, (w_b, w_c)\n",
    "\n",
    "def map_baby_y_to_union(y_baby_012):\n",
    "    y_baby_012 = np.asarray(y_baby_012).astype(int)\n",
    "    return np.array([union_idx[BABY_LABELS_3[i]] for i in y_baby_012], dtype=int)\n",
    "\n",
    "def map_china_y_to_union(y_china_012):\n",
    "    y_china_012 = np.asarray(y_china_012).astype(int)\n",
    "    return np.array([union_idx[CHINA_LABELS_3[i]] for i in y_china_012], dtype=int)\n",
    "\n",
    "\n",
    "def evaluate_union_subset(y_true_union, y_pred_union, subset_names, name=\"\"):\n",
    "    subset_ids = [union_idx[n] for n in subset_names]\n",
    "\n",
    "    acc = accuracy_score(y_true_union, y_pred_union)\n",
    "    f1m = f1_score(y_true_union, y_pred_union, average=\"macro\", labels=subset_ids)\n",
    "    f1w = f1_score(y_true_union, y_pred_union, average=\"weighted\", labels=subset_ids)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"✅ {name}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Subset:\", subset_names)\n",
    "    print(f\"Accuracy: {acc:.4f} | F1-macro: {f1m:.4f} | F1-weighted: {f1w:.4f}\")\n",
    "\n",
    "    print(\"\\nReport:\")\n",
    "    print(classification_report(\n",
    "        y_true_union, y_pred_union,\n",
    "        labels=subset_ids,\n",
    "        target_names=subset_names,\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(confusion_matrix(y_true_union, y_pred_union, labels=subset_ids))\n",
    "\n",
    "    return {\"acc\": acc, \"f1_macro\": f1m, \"f1_weighted\": f1w}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741bf65-1051-40e3-85a0-377bea4d06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# BABY TEST\n",
    "# ----------------------------\n",
    "X_test_baby = X_test0\n",
    "y_test_baby = y_test0   # {0,1,2} => BABY_LABELS_3\n",
    "\n",
    "if X_test_baby.ndim == 3:\n",
    "    X_test_baby = np.expand_dims(X_test_baby, axis=-1)\n",
    "\n",
    "baby_logits = get_logits_keras(baby_logits_model, X_test_baby)\n",
    "china_logits = get_logits_keras(china_logits_model, X_test_baby)\n",
    "\n",
    "print(\"baby_logits:\", baby_logits.shape, \"china_logits:\", china_logits.shape)\n",
    "\n",
    "p_union_baby, y_pred_union_baby, (wb_baby, wc_baby) = fuse_union_3v3(\n",
    "    baby_logits, china_logits,\n",
    "    T_B=T_B, T_C=T_C,\n",
    "    tau=1.0\n",
    ")\n",
    "\n",
    "y_true_union_baby = map_baby_y_to_union(y_test_baby)\n",
    "\n",
    "baby_results = evaluate_union_subset(\n",
    "    y_true_union_baby,\n",
    "    y_pred_union_baby,\n",
    "    subset_names=[\"Hungry\",\"Sleepy\",\"Wakeup\"],\n",
    "    name=\"Ensemble on BABY test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e4d93475-ed2b-4d5f-95be-d523c5c009c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_baby: (105, 100, 280, 1)\n",
      "y_test_baby: (105,)\n",
      "baby_logits: (105, 3)\n",
      "china_logits: (105, 3)\n",
      "y_pred_union_baby: (105,)\n",
      "p_union_baby: (105, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test_baby:\", X_test_baby.shape)\n",
    "print(\"y_test_baby:\", np.asarray(y_test_baby).shape)\n",
    "print(\"baby_logits:\", baby_logits.shape)\n",
    "print(\"china_logits:\", china_logits.shape)\n",
    "print(\"y_pred_union_baby:\", np.asarray(y_pred_union_baby).shape)\n",
    "print(\"p_union_baby:\", np.asarray(p_union_baby).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ffa8e-677e-47ba-b8ef-b025281c569a",
   "metadata": {},
   "source": [
    "# Energy gated Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d556054c-e3b3-4b8e-94fe-7ad97f46668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_from_logits(z, T=1.0):\n",
    "    \"\"\"\n",
    "    z: [N,C] logits\n",
    "    returns energy: [N]\n",
    "    \"\"\"\n",
    "    zT = z / float(T)\n",
    "    m = np.max(zT, axis=1, keepdims=True)\n",
    "    lse = m + np.log(np.sum(np.exp(zT - m), axis=1, keepdims=True) + 1e-12)\n",
    "    return -lse.squeeze(1)   # [N]\n",
    "\n",
    "def energy_weights(zB, zC, T_B, T_C, gamma=1.0):\n",
    "    \"\"\"\n",
    "    zB: [N,3] baby logits (raw logits BEFORE dividing by T)|\n",
    "    zC: [N,3] china logits (raw logits BEFORE dividing by T)\n",
    "    returns wb,wc in [N]\n",
    "    \"\"\"\n",
    "    Eb = energy_from_logits(zB, T_B)\n",
    "    Ec = energy_from_logits(zC, T_C)\n",
    "    wb = np.exp(-gamma * Eb)\n",
    "    wc = np.exp(-gamma * Ec)\n",
    "    s = wb + wc + 1e-12\n",
    "    return wb/s, wc/s, Eb, Ec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b44ff96-535d-4f98-a17b-59e0a7955e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_union_3v3_openworld(\n",
    "    baby_logits_3,\n",
    "    china_logits_3,\n",
    "    T_B, T_C,\n",
    "    gamma=1.0\n",
    "):\n",
    "    N = baby_logits_3.shape[0]\n",
    "    z_union = np.full((N, U), -1e30, dtype=np.float32)\n",
    "\n",
    "    # calibrated logits\n",
    "    zB = baby_logits_3 / float(T_B)\n",
    "    zC = china_logits_3 / float(T_C)\n",
    "\n",
    "    # weights from ENERGY (not entropy)\n",
    "    wb, wc, Eb, Ec = energy_weights(baby_logits_3, china_logits_3, T_B, T_C, gamma=gamma)\n",
    "\n",
    "    print(\"Mean energy baby :\", Eb.mean())\n",
    "    print(\"Mean energy china:\", Ec.mean())\n",
    "    print(\"Mean w_b:\", wb.mean(), \"Mean w_c:\", wc.mean())\n",
    "    print(\"Frac w_c > 0.5:\", np.mean(wc > 0.5))\n",
    "\n",
    "    # fill disjoint\n",
    "    for k, name in enumerate(BABY_LABELS_3):\n",
    "        z_union[:, union_idx[name]] = zB[:, k]\n",
    "    for k, name in enumerate(CHINA_LABELS_3):\n",
    "        z_union[:, union_idx[name]] = zC[:, k]\n",
    "\n",
    "    # fuse shared Sleepy\n",
    "    sleepy_u = union_idx[\"Sleepy\"]\n",
    "    zB_sleepy = zB[:, BABY_LABELS_3.index(\"Sleepy\")]\n",
    "    zC_sleepy = zC[:, CHINA_LABELS_3.index(\"Sleepy\")]\n",
    "    z_union[:, sleepy_u] = np.log(wc*np.exp(zC_sleepy) + wb*np.exp(zB_sleepy) + 1e-12)\n",
    "\n",
    "    # softmax over union\n",
    "    z = z_union - np.max(z_union, axis=1, keepdims=True)\n",
    "    p_union = np.exp(z) / (np.sum(np.exp(z), axis=1, keepdims=True) + 1e-12)\n",
    "    y_pred_union = np.argmax(p_union, axis=1)\n",
    "\n",
    "    return p_union, y_pred_union, (wb, wc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ff611a5-3f39-41f5-b15d-98dcc522a71b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baby_logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1347034/4192222827.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m p_union_baby, y_pred_union_baby, (wb_baby, wc_baby) = fuse_union_3v3_openworld(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbaby_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchina_logits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mT_B\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_C\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_C\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m   \u001b[0;31m# try 1.0, 2.0, 5.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baby_logits' is not defined"
     ]
    }
   ],
   "source": [
    "p_union_baby, y_pred_union_baby, (wb_baby, wc_baby) = fuse_union_3v3_openworld(\n",
    "    baby_logits, china_logits,\n",
    "    T_B=T_B, T_C=T_C,\n",
    "    gamma=2.0   # try 1.0, 2.0, 5.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9219df9-cb25-4fc9-9c93-11617467968f",
   "metadata": {},
   "source": [
    "# Masked Fusion evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fffbc31-d4a8-4902-b983-6fb975c87d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "\n",
    "def eval_ensemble_on_baby_fold0(\n",
    "    y_test0,                 # shape [N], integers like 0/1/2 in fold mapping\n",
    "    label_to_class,          # dict: {0:'Hungry',1:'Sleepy',2:'Wakeup'}\n",
    "    p_union_baby,            # [N,5] union probs\n",
    "    y_pred_union_baby,       # [N] union argmax\n",
    "    UNION_LABELS=(\"Diaper\",\"Uncomfortable\",\"Sleepy\",\"Hungry\",\"Wakeup\"),\n",
    "):\n",
    "    N = len(y_test0)\n",
    "    y_test0 = np.asarray(y_test0).astype(int)\n",
    "    y_pred_union_baby = np.asarray(y_pred_union_baby).astype(int)\n",
    "    p_union_baby = np.asarray(p_union_baby)\n",
    "\n",
    "    assert p_union_baby.shape[0] == N, f\"p_union_baby N mismatch: {p_union_baby.shape[0]} vs {N}\"\n",
    "    assert y_pred_union_baby.shape[0] == N, f\"y_pred_union_baby N mismatch: {y_pred_union_baby.shape[0]} vs {N}\"\n",
    "\n",
    "    union_idx = {n:i for i,n in enumerate(UNION_LABELS)}\n",
    "\n",
    "    # ---- map y_true (fold ids) -> union ids using label_to_class ----\n",
    "    y_true_names = np.array([label_to_class[int(i)] for i in y_test0], dtype=object)\n",
    "    y_true_union = np.array([union_idx[name] for name in y_true_names], dtype=int)\n",
    "\n",
    "    # ---- restrict predictions to baby classes only (safety) ----\n",
    "    baby_names = [\"Hungry\", \"Sleepy\", \"Wakeup\"]\n",
    "    baby_union_ids = np.array([union_idx[n] for n in baby_names], dtype=int)\n",
    "\n",
    "    # Convert union prediction -> baby local index (0/1/2) by:\n",
    "    # 1) take probs on baby union ids only\n",
    "    p_baby = p_union_baby[:, baby_union_ids]                 # [N,3] in order [Hungry,Sleepy,Wakeup]\n",
    "    y_pred_local = np.argmax(p_baby, axis=1)                 # 0..2 in that order\n",
    "    # map local -> class name\n",
    "    id2name_local = {0:\"Hungry\", 1:\"Sleepy\", 2:\"Wakeup\"}\n",
    "    y_pred_names = np.array([id2name_local[int(i)] for i in y_pred_local], dtype=object)\n",
    "\n",
    "    # Now map y_true_names to same local space for metrics\n",
    "    name2id_local = {\"Hungry\":0,\"Sleepy\":1,\"Wakeup\":2}\n",
    "    y_true_local = np.array([name2id_local[n] for n in y_true_names], dtype=int)\n",
    "\n",
    "    # ---- metrics in Baby local space ----\n",
    "    acc = accuracy_score(y_true_local, y_pred_local)\n",
    "    f1_macro = f1_score(y_true_local, y_pred_local, average=\"macro\")\n",
    "    f1_weighted = f1_score(y_true_local, y_pred_local, average=\"weighted\")\n",
    "    mcc = matthews_corrcoef(y_true_local, y_pred_local)\n",
    "    cm = confusion_matrix(y_true_local, y_pred_local, labels=[0,1,2])\n",
    "\n",
    "    # ---- AUC (macro OVR) ----\n",
    "    y_true_onehot = np.eye(3)[y_true_local]  # [N,3]\n",
    "    auc_macro = roc_auc_score(y_true_onehot, p_baby, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ Ensemble on Baby2020 (Fold0) — evaluated correctly\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"N            : {N}\")\n",
    "    print(f\"Accuracy     : {acc:.4f}\")\n",
    "    print(f\"F1 (macro)   : {f1_macro:.4f}\")\n",
    "    print(f\"F1 (weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"MCC          : {mcc:.4f}\")\n",
    "    print(f\"AUC (macro)  : {auc_macro:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification report (Baby classes):\")\n",
    "    print(classification_report(\n",
    "        y_true_local, y_pred_local,\n",
    "        labels=[0,1,2],\n",
    "        target_names=[\"Hungry\",\"Sleepy\",\"Wakeup\"],\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    print(\"Confusion matrix (rows=true, cols=pred) order=[Hungry,Sleepy,Wakeup]:\")\n",
    "    print(cm)\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc, \"f1_macro\": f1_macro, \"f1_weighted\": f1_weighted,\n",
    "        \"mcc\": mcc, \"auc_macro\": auc_macro, \"cm\": cm,\n",
    "        \"y_true_local\": y_true_local, \"y_pred_local\": y_pred_local,\n",
    "        \"p_baby\": p_baby\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e6cb59d-a0dc-4e8b-8de8-e93ec63f95a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p_union_baby' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1347034/2522675274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my_test0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlabel_to_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_to_class\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# <- from your fold0 split output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mp_union_baby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_union_baby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my_pred_union_baby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred_union_baby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p_union_baby' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class_to_label= {'Hungry': 1, 'Sleepy': 0, 'Wakeup': 2}\n",
    "label_to_class= {1: 'Hungry', 0: 'Sleepy', 2: 'Wakeup'}\n",
    "\n",
    "\n",
    "baby_metrics = eval_ensemble_on_baby_fold0(\n",
    "    y_test0=y_test0,\n",
    "    label_to_class=label_to_class,   # <- from your fold0 split output\n",
    "    p_union_baby=p_union_baby,\n",
    "    y_pred_union_baby=y_pred_union_baby\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e907e9-f89c-49f6-ad39-122f1e89cff2",
   "metadata": {},
   "source": [
    "# Print the weights to see if Chinese dominates on Baby data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366cc2fd-f279-47c6-bdd6-8fb23e172bc5",
   "metadata": {},
   "source": [
    "## If wc_baby is often high on Baby test, that explains the degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e89cf674-c15c-4ac8-baa4-66032871afac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wb_baby' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1347034/1699508061.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean w_b (baby weight):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb_baby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean w_c (china weight):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc_baby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fraction wc>0.5:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc_baby\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wb_baby' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Mean w_b (baby weight):\", np.mean(wb_baby))\n",
    "print(\"Mean w_c (china weight):\", np.mean(wc_baby))\n",
    "print(\"Fraction wc>0.5:\", np.mean(wc_baby > 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e37bd3-e929-433b-920e-800ff7e367dd",
   "metadata": {},
   "source": [
    "# Now test Ensemble posterior Fusion on Chinese baby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43bd1c00-8915-4ed0-bd78-d33b70429d37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_logits_keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1347034/3252846116.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX_test_china\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_china\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mbaby_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logits_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaby_logits_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_china\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mchina_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logits_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchina_logits_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_china\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_logits_keras' is not defined"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# CHINESE TEST\n",
    "# ----------------------------\n",
    "X_test_china = X_val_Chinese_babyCry\n",
    "y_test_china = y_val_Chinese_babyCry  # {0,1,2} => CHINA_LABELS_3\n",
    "\n",
    "if X_test_china.ndim == 3:\n",
    "    X_test_china = np.expand_dims(X_test_china, axis=-1)\n",
    "\n",
    "baby_logits = get_logits_keras(baby_logits_model, X_test_china)\n",
    "china_logits = get_logits_keras(china_logits_model, X_test_china)\n",
    "\n",
    "print(\"baby_logits:\", baby_logits.shape, \"china_logits:\", china_logits.shape)\n",
    "\n",
    "p_union_china, y_pred_union_china, (wb_china, wc_china) = fuse_union_3v3(\n",
    "    baby_logits, china_logits,\n",
    "    T_B=T_B, T_C=T_C,\n",
    "    tau=1.0\n",
    ")\n",
    "\n",
    "y_true_union_china = map_china_y_to_union(y_test_china)\n",
    "\n",
    "china_results = evaluate_union_subset(\n",
    "    y_true_union_china,\n",
    "    y_pred_union_china,\n",
    "    subset_names=[\"Diaper\",\"Sleepy\",\"Uncomfortable\"],\n",
    "    name=\"Ensemble on CHINESE test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a50b1-d2f3-4a53-911c-38b5b8be87db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08260f-f168-4b49-86bf-3428c3e06188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c546516-43b0-418b-ae12-e330bff86bd4",
   "metadata": {},
   "source": [
    "# Domain-gate head that learns to predict “this input looks like Baby2020-domain vs Chinese-domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09bac4f1-3f14-4034-815c-b53954e7478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def softmax_np(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / (np.sum(e, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def entropy_np(p):\n",
    "    return -np.sum(p * np.log(p + 1e-12), axis=1)\n",
    "\n",
    "def energy_np(logits, T=1.0):\n",
    "    z = logits / float(T)\n",
    "    m = np.max(z, axis=1, keepdims=True)\n",
    "    lse = m + np.log(np.sum(np.exp(z - m), axis=1, keepdims=True) + 1e-12)\n",
    "    return -lse.squeeze(1)  # [N]\n",
    "\n",
    "def top2_margin_np(logits):\n",
    "    # margin between top1 and top2 logits\n",
    "    part = np.partition(logits, -2, axis=1)\n",
    "    top2 = part[:, -2]\n",
    "    top1 = part[:, -1]\n",
    "    return top1 - top2\n",
    "\n",
    "def get_logits_keras(logits_model, X, batch_size=64):\n",
    "    return logits_model.predict(X, batch_size=batch_size, verbose=0).astype(np.float32)\n",
    "\n",
    "def logits_stats_features(logits, T=1.0):\n",
    "    \"\"\"\n",
    "    logits: [N,C]\n",
    "    returns features: [N,F]\n",
    "    \"\"\"\n",
    "    p = softmax_np(logits / float(T))\n",
    "    H = entropy_np(p)                          # [N]\n",
    "    mp = np.max(p, axis=1)                     # [N]\n",
    "    margin = top2_margin_np(logits / float(T)) # [N]\n",
    "    E = energy_np(logits, T=T)                 # [N]\n",
    "    l2 = np.linalg.norm(logits, axis=1)        # [N]\n",
    "    zmax = np.max(logits, axis=1)              # [N]\n",
    "    zmean = np.mean(logits, axis=1)            # [N]\n",
    "    zstd = np.std(logits, axis=1)              # [N]\n",
    "\n",
    "    # stack\n",
    "    return np.stack([H, mp, margin, E, l2, zmax, zmean, zstd], axis=1).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fc0363b-cc8b-480d-a6dd-3c3f4260a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gate_dataset(\n",
    "    X_baby, X_china,\n",
    "    baby_logits_model, china_logits_model,\n",
    "    T_B=1.0, T_C=1.0,\n",
    "    batch_size=64\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_gate: [N, F]  features for gate\n",
    "      y_dom : [N]     domain labels (0=baby, 1=china)\n",
    "    \"\"\"\n",
    "    # ensure 4D\n",
    "    if X_baby.ndim == 3:  X_baby  = np.expand_dims(X_baby, axis=-1)\n",
    "    if X_china.ndim == 3: X_china = np.expand_dims(X_china, axis=-1)\n",
    "\n",
    "    # ---- logits on baby-domain data ----\n",
    "    zB_on_baby = get_logits_keras(baby_logits_model,  X_baby,  batch_size=batch_size)\n",
    "    zC_on_baby = get_logits_keras(china_logits_model, X_baby,  batch_size=batch_size)\n",
    "\n",
    "    fB_on_baby = logits_stats_features(zB_on_baby, T=T_B)\n",
    "    fC_on_baby = logits_stats_features(zC_on_baby, T=T_C)\n",
    "\n",
    "    # include difference features (helps a lot)\n",
    "    fDiff_baby = (fB_on_baby - fC_on_baby)\n",
    "\n",
    "    X_gate_baby = np.concatenate([fB_on_baby, fC_on_baby, fDiff_baby], axis=1)\n",
    "    y_dom_baby = np.zeros(len(X_gate_baby), dtype=np.int64)\n",
    "\n",
    "    # ---- logits on china-domain data ----\n",
    "    zB_on_china = get_logits_keras(baby_logits_model,  X_china, batch_size=batch_size)\n",
    "    zC_on_china = get_logits_keras(china_logits_model, X_china, batch_size=batch_size)\n",
    "\n",
    "    fB_on_china = logits_stats_features(zB_on_china, T=T_B)\n",
    "    fC_on_china = logits_stats_features(zC_on_china, T=T_C)\n",
    "    fDiff_china = (fB_on_china - fC_on_china)\n",
    "\n",
    "    X_gate_china = np.concatenate([fB_on_china, fC_on_china, fDiff_china], axis=1)\n",
    "    y_dom_china = np.ones(len(X_gate_china), dtype=np.int64)\n",
    "\n",
    "    # ---- combine ----\n",
    "    X_gate = np.concatenate([X_gate_baby, X_gate_china], axis=0)\n",
    "    y_dom  = np.concatenate([y_dom_baby,  y_dom_china],  axis=0)\n",
    "\n",
    "    # shuffle\n",
    "    idx = np.random.permutation(len(X_gate))\n",
    "    return X_gate[idx], y_dom[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de63c7ef-cd29-4d85-8b10-5121df33ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "def train_domain_gate(X_gate, y_dom):\n",
    "    Xtr, Xva, ytr, yva = train_test_split(\n",
    "        X_gate, y_dom, test_size=0.25, random_state=42, stratify=y_dom\n",
    "    )\n",
    "\n",
    "    gate = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "    gate.fit(Xtr, ytr)\n",
    "\n",
    "    p_va = gate.predict_proba(Xva)[:, 1]  # P(domain=china)\n",
    "    yhat = (p_va >= 0.5).astype(int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ Domain gate validation\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Acc:\", accuracy_score(yva, yhat))\n",
    "    print(\"AUC:\", roc_auc_score(yva, p_va))\n",
    "    print(\"\\nReport:\")\n",
    "    print(classification_report(yva, yhat, digits=4))\n",
    "\n",
    "    return gate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "350ffeca-9c76-49d0-a3f4-284c0e2556e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gate_weights_from_proba(p_china, alpha=1.0):\n",
    "    \"\"\"\n",
    "    p_china: [N] probability input is China-domain\n",
    "    alpha: >1 makes gate more decisive, <1 makes softer\n",
    "    \"\"\"\n",
    "    p = np.clip(p_china, 1e-6, 1-1e-6)\n",
    "    # logit transform + scaling\n",
    "    logit = np.log(p/(1-p))\n",
    "    p2 = 1.0 / (1.0 + np.exp(-alpha * logit))\n",
    "    w_c = p2\n",
    "    w_b = 1.0 - p2\n",
    "    return w_b.astype(np.float32), w_c.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea0a635e-641d-4ae9-ab0e-80c7ea69980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNION_LABELS  = [\"Diaper\", \"Uncomfortable\", \"Sleepy\", \"Hungry\", \"Wakeup\"]\n",
    "BABY_LABELS_3 = [\"Hungry\", \"Sleepy\",  \"Wakeup\"]\n",
    "CHINA_LABELS_3 = [\"Diaper\", \"Sleepy\", \"Uncomfortable\"]\n",
    "\n",
    "U = len(UNION_LABELS)\n",
    "union_idx = {n:i for i,n in enumerate(UNION_LABELS)}\n",
    "\n",
    "def fuse_union_with_domain_gate(\n",
    "    baby_logits_3, china_logits_3,\n",
    "    T_B, T_C,\n",
    "    wb, wc\n",
    "):\n",
    "    \"\"\"\n",
    "    wb,wc: [N] from domain gate\n",
    "    returns p_union [N,5], y_pred_union [N]\n",
    "    \"\"\"\n",
    "    N = baby_logits_3.shape[0]\n",
    "    z_union = np.full((N, U), -1e30, dtype=np.float32)\n",
    "\n",
    "    # calibrated logits\n",
    "    zB = baby_logits_3 / float(T_B)\n",
    "    zC = china_logits_3 / float(T_C)\n",
    "\n",
    "    # fill disjoint\n",
    "    for k, name in enumerate(BABY_LABELS_3):\n",
    "        z_union[:, union_idx[name]] = zB[:, k]\n",
    "    for k, name in enumerate(CHINA_LABELS_3):\n",
    "        z_union[:, union_idx[name]] = zC[:, k]\n",
    "\n",
    "    # fuse shared Sleepy only (log-sum-exp)\n",
    "    sleepy_u = union_idx[\"Sleepy\"]\n",
    "    zB_sleepy = zB[:, BABY_LABELS_3.index(\"Sleepy\")]\n",
    "    zC_sleepy = zC[:, CHINA_LABELS_3.index(\"Sleepy\")]\n",
    "    z_union[:, sleepy_u] = np.log(wc*np.exp(zC_sleepy) + wb*np.exp(zB_sleepy) + 1e-12)\n",
    "\n",
    "    # softmax over union\n",
    "    z = z_union - np.max(z_union, axis=1, keepdims=True)\n",
    "    p_union = np.exp(z) / (np.sum(np.exp(z), axis=1, keepdims=True) + 1e-12)\n",
    "    y_pred_union = np.argmax(p_union, axis=1)\n",
    "\n",
    "    return p_union, y_pred_union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c202d1a-3622-40d4-9500-c6493d66faa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T_B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1347034/2003787761.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbaby_logits_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaby_logits_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mchina_logits_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchina_logits_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mT_B\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_C\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T_B' is not defined"
     ]
    }
   ],
   "source": [
    "# Build gate training features using unlabeled domain splits\n",
    "X_gate, y_dom = build_gate_dataset(\n",
    "    X_baby=X_test0,                 # or a baby val split if you have one\n",
    "    X_china=X_val_Chinese_babyCry,   # china val split\n",
    "    baby_logits_model=baby_logits_model,\n",
    "    china_logits_model=china_logits_model,\n",
    "    T_B=T_B, T_C=T_C\n",
    ")\n",
    "\n",
    "gate = train_domain_gate(X_gate, y_dom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25d033bf-2888-4175-b735-ad12797431df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x14bebc088550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 18 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x14bebc088550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'T_B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1347034/1092126966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# gate features for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_stats_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mfC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_stats_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfB\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T_B' is not defined"
     ]
    }
   ],
   "source": [
    "# logits on baby test\n",
    "X_baby = X_test0\n",
    "if X_baby.ndim == 3: X_baby = np.expand_dims(X_baby, axis=-1)\n",
    "\n",
    "zB = get_logits_keras(baby_logits_model,  X_baby)\n",
    "zC = get_logits_keras(china_logits_model, X_baby)\n",
    "\n",
    "# gate features for this batch\n",
    "fB = logits_stats_features(zB, T=T_B)\n",
    "fC = logits_stats_features(zC, T=T_C)\n",
    "fD = fB - fC\n",
    "Xg = np.concatenate([fB, fC, fD], axis=1)\n",
    "\n",
    "# gate proba + weights\n",
    "p_china = gate.predict_proba(Xg)[:, 1]      # P(china-domain)\n",
    "wb, wc = gate_weights_from_proba(p_china, alpha=2.0)\n",
    "\n",
    "print(\"Mean wc (china weight) on BABY test:\", wc.mean())\n",
    "print(\"Frac wc>0.5:\", np.mean(wc > 0.5))\n",
    "\n",
    "# fuse open-world (5 classes)\n",
    "p_union_baby, y_pred_union_baby = fuse_union_with_domain_gate(\n",
    "    baby_logits_3=zB, china_logits_3=zC,\n",
    "    T_B=T_B, T_C=T_C,\n",
    "    wb=wb, wc=wc\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821db1f-1663-4abf-b3ba-b5dad99bac50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269ba41-fc1e-48f4-94c4-de73a4ee3eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f586f54-f1bb-4547-8fb2-fbd46e92ae7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20d77176-0dbe-4de9-b310-c6380aa9f2b8",
   "metadata": {},
   "source": [
    "# More powerful gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40bf1f10-f109-4871-b530-cf8f6598b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, confusion_matrix,\n",
    "    classification_report, roc_auc_score\n",
    ")\n",
    "\n",
    "\n",
    "UNION_LABELS  = [\"Diaper\", \"Uncomfortable\", \"Sleepy\", \"Hungry\", \"Wakeup\"]\n",
    "BABY_LABELS_3 = [\"Hungry\", \"Sleepy\", \"Wakeup\"]\n",
    "CHINA_LABELS_3 = [\"Diaper\", \"Sleepy\", \"Uncomfortable\"]  # choice B\n",
    "\n",
    "U = len(UNION_LABELS)\n",
    "union_idx = {n:i for i,n in enumerate(UNION_LABELS)}\n",
    "\n",
    "BABY_UNION_IDS  = [union_idx[n] for n in BABY_LABELS_3]\n",
    "CHINA_UNION_IDS = [union_idx[n] for n in CHINA_LABELS_3]\n",
    "\n",
    "def map_baby_y_to_union(y012):\n",
    "    y012 = np.asarray(y012).astype(int)\n",
    "    return np.array([union_idx[BABY_LABELS_3[i]] for i in y012], dtype=int)\n",
    "\n",
    "def map_china_y_to_union(y012):\n",
    "    y012 = np.asarray(y012).astype(int)\n",
    "    return np.array([union_idx[CHINA_LABELS_3[i]] for i in y012], dtype=int)\n",
    "\n",
    "\n",
    "def softmax_np(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / (np.sum(e, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def entropy_np(p):\n",
    "    return -np.sum(p * np.log(p + 1e-12), axis=1)\n",
    "\n",
    "def energy_np(logits, T=1.0):\n",
    "    z = logits / float(T)\n",
    "    m = np.max(z, axis=1, keepdims=True)\n",
    "    lse = m + np.log(np.sum(np.exp(z - m), axis=1, keepdims=True) + 1e-12)\n",
    "    return -lse.squeeze(1)\n",
    "\n",
    "def top2_margin_np(logits):\n",
    "    part = np.partition(logits, -2, axis=1)\n",
    "    top2 = part[:, -2]\n",
    "    top1 = part[:, -1]\n",
    "    return top1 - top2\n",
    "\n",
    "def get_logits_keras(logits_model, X, batch_size=64):\n",
    "    return logits_model.predict(X, batch_size=batch_size, verbose=0).astype(np.float32)\n",
    "\n",
    "def logits_stats_features(logits, T=1.0):\n",
    "    \"\"\"\n",
    "    logits: [N,C]\n",
    "    returns: [N,8]\n",
    "    \"\"\"\n",
    "    z = logits / float(T)\n",
    "    p = softmax_np(z)\n",
    "    H = entropy_np(p)\n",
    "    mp = np.max(p, axis=1)\n",
    "    margin = top2_margin_np(z)\n",
    "    E = energy_np(logits, T=T)\n",
    "    l2 = np.linalg.norm(logits, axis=1)\n",
    "    zmax = np.max(logits, axis=1)\n",
    "    zmean = np.mean(logits, axis=1)\n",
    "    zstd = np.std(logits, axis=1)\n",
    "    return np.stack([H, mp, margin, E, l2, zmax, zmean, zstd], axis=1).astype(np.float32)\n",
    "\n",
    "def build_gate_features_for_batch(zB, zC, T_B=1.0, T_C=1.0):\n",
    "    fB = logits_stats_features(zB, T=T_B)\n",
    "    fC = logits_stats_features(zC, T=T_C)\n",
    "    fD = fB - fC\n",
    "    return np.concatenate([fB, fC, fD], axis=1).astype(np.float32)  # [N, 24]\n",
    "\n",
    "def build_gate_dataset(\n",
    "    X_baby, X_china,\n",
    "    baby_logits_model, china_logits_model,\n",
    "    T_B=1.0, T_C=1.0,\n",
    "    batch_size=64\n",
    "):\n",
    "    if X_baby.ndim == 3:  X_baby  = np.expand_dims(X_baby, axis=-1)\n",
    "    if X_china.ndim == 3: X_china = np.expand_dims(X_china, axis=-1)\n",
    "\n",
    "    zB_b = get_logits_keras(baby_logits_model,  X_baby,  batch_size=batch_size)\n",
    "    zC_b = get_logits_keras(china_logits_model, X_baby,  batch_size=batch_size)\n",
    "    Xg_b = build_gate_features_for_batch(zB_b, zC_b, T_B=T_B, T_C=T_C)\n",
    "    yg_b = np.zeros(len(Xg_b), dtype=np.int64)\n",
    "\n",
    "    zB_c = get_logits_keras(baby_logits_model,  X_china, batch_size=batch_size)\n",
    "    zC_c = get_logits_keras(china_logits_model, X_china, batch_size=batch_size)\n",
    "    Xg_c = build_gate_features_for_batch(zB_c, zC_c, T_B=T_B, T_C=T_C)\n",
    "    yg_c = np.ones(len(Xg_c), dtype=np.int64)\n",
    "\n",
    "    Xg = np.concatenate([Xg_b, Xg_c], axis=0)\n",
    "    yg = np.concatenate([yg_b, yg_c], axis=0)\n",
    "\n",
    "    idx = np.random.permutation(len(Xg))\n",
    "    return Xg[idx], yg[idx]\n",
    "\n",
    "\n",
    "def make_mlp_gate(input_dim, hidden=(64, 32), dropout=0.2, lr=1e-3):\n",
    "    inp = tf.keras.Input(shape=(input_dim,))\n",
    "    x = tf.keras.layers.LayerNormalization()(inp)\n",
    "\n",
    "    for h in hidden:\n",
    "        x = tf.keras.layers.Dense(h, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    # output: P(domain=china)\n",
    "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, out, name=\"domain_gate_mlp\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(name=\"acc\"),\n",
    "            tf.keras.metrics.AUC(name=\"auc\")\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_domain_gate_mlp(Xg, yg, epochs=50, batch_size=64):\n",
    "    Xtr, Xva, ytr, yva = train_test_split(\n",
    "        Xg, yg, test_size=0.25, random_state=42, stratify=yg\n",
    "    )\n",
    "\n",
    "    gate = make_mlp_gate(input_dim=Xg.shape[1], hidden=(64, 32), dropout=0.2, lr=1e-3)\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_auc\", mode=\"max\", patience=8, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "\n",
    "    gate.fit(\n",
    "        Xtr, ytr,\n",
    "        validation_data=(Xva, yva),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "\n",
    "    pva = gate.predict(Xva, batch_size=batch_size, verbose=0).squeeze()\n",
    "    yhat = (pva >= 0.5).astype(int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ Domain gate (MLP) validation\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Acc:\", accuracy_score(yva, yhat))\n",
    "    print(\"AUC:\", roc_auc_score(yva, pva))\n",
    "    print(classification_report(yva, yhat, digits=4))\n",
    "    return gate\n",
    "\n",
    "\n",
    "def gate_weights_from_pchina(p_china, alpha=2.0):\n",
    "    \"\"\"\n",
    "    alpha>1 => more decisive gate, alpha<1 => softer\n",
    "    \"\"\"\n",
    "    p = np.clip(p_china, 1e-6, 1 - 1e-6)\n",
    "    logit = np.log(p/(1-p))\n",
    "    p2 = 1.0 / (1.0 + np.exp(-alpha * logit))\n",
    "    wc = p2.astype(np.float32)          # china weight\n",
    "    wb = (1.0 - p2).astype(np.float32)  # baby weight\n",
    "    return wb, wc\n",
    "\n",
    "def fuse_union_openworld(\n",
    "    baby_logits_3, china_logits_3,\n",
    "    T_B, T_C,\n",
    "    wb, wc\n",
    "):\n",
    "    N = baby_logits_3.shape[0]\n",
    "    z_union = np.full((N, U), -1e30, dtype=np.float32)\n",
    "\n",
    "    zB = baby_logits_3 / float(T_B)\n",
    "    zC = china_logits_3 / float(T_C)\n",
    "\n",
    "    # fill disjoint\n",
    "    for k, name in enumerate(BABY_LABELS_3):\n",
    "        z_union[:, union_idx[name]] = zB[:, k]\n",
    "    for k, name in enumerate(CHINA_LABELS_3):\n",
    "        z_union[:, union_idx[name]] = zC[:, k]\n",
    "\n",
    "    # fuse Sleepy only\n",
    "    sleepy_u = union_idx[\"Sleepy\"]\n",
    "    zB_sleepy = zB[:, BABY_LABELS_3.index(\"Sleepy\")]\n",
    "    zC_sleepy = zC[:, CHINA_LABELS_3.index(\"Sleepy\")]\n",
    "    z_union[:, sleepy_u] = np.log(wc*np.exp(zC_sleepy) + wb*np.exp(zB_sleepy) + 1e-12)\n",
    "\n",
    "    # final softmax over 5 classes\n",
    "    z = z_union - np.max(z_union, axis=1, keepdims=True)\n",
    "    p_union = np.exp(z) / (np.sum(np.exp(z), axis=1, keepdims=True) + 1e-12)\n",
    "    y_pred = np.argmax(p_union, axis=1)\n",
    "    return p_union, y_pred\n",
    "\n",
    "\n",
    "def eval_gate_on_dataset(\n",
    "    X, true_domain,   # true_domain: 0 for baby, 1 for china\n",
    "    baby_logits_model, china_logits_model,\n",
    "    gate_model,\n",
    "    T_B, T_C,\n",
    "    alpha=2.0,\n",
    "    name=\"\"\n",
    "):\n",
    "    if X.ndim == 3: X = np.expand_dims(X, axis=-1)\n",
    "\n",
    "    zB = get_logits_keras(baby_logits_model, X)\n",
    "    zC = get_logits_keras(china_logits_model, X)\n",
    "    Xg = build_gate_features_for_batch(zB, zC, T_B=T_B, T_C=T_C)\n",
    "\n",
    "    p_china = gate_model.predict(Xg, batch_size=64, verbose=0).squeeze()\n",
    "    yhat_dom = (p_china >= 0.5).astype(int)\n",
    "\n",
    "    wb, wc = gate_weights_from_pchina(p_china, alpha=alpha)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"✅ Gate check: {name}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"True domain={true_domain}  |  Pred-dom acc={accuracy_score(np.full_like(yhat_dom,true_domain), yhat_dom):.4f}\")\n",
    "    print(f\"Mean wb={wb.mean():.4f}  Mean wc={wc.mean():.4f}  Frac wc>0.5={np.mean(wc>0.5):.4f}\")\n",
    "\n",
    "    return zB, zC, wb, wc, p_china, yhat_dom\n",
    "\n",
    "def eval_ensemble_subset(y_true_union, y_pred_union, subset_union_ids, subset_names, name=\"\"):\n",
    "    acc = accuracy_score(y_true_union, y_pred_union)\n",
    "    f1m = f1_score(y_true_union, y_pred_union, labels=subset_union_ids, average=\"macro\")\n",
    "    f1w = f1_score(y_true_union, y_pred_union, labels=subset_union_ids, average=\"weighted\")\n",
    "    mcc = matthews_corrcoef(y_true_union, y_pred_union)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"✅ Ensemble metrics: {name}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Accuracy(all union preds vs union true): {acc:.4f}\")\n",
    "    print(f\"F1-macro(subset): {f1m:.4f}\")\n",
    "    print(f\"F1-weighted(subset): {f1w:.4f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "\n",
    "    print(\"\\nReport (subset labels only):\")\n",
    "    print(classification_report(\n",
    "        y_true_union, y_pred_union,\n",
    "        labels=subset_union_ids,\n",
    "        target_names=subset_names,\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    print(\"Confusion matrix (subset):\")\n",
    "    print(confusion_matrix(y_true_union, y_pred_union, labels=subset_union_ids))\n",
    "\n",
    "    return {\"acc\": acc, \"f1_macro\": f1m, \"f1_weighted\": f1w, \"mcc\": mcc}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "340d0148-d133-4cb6-85ad-40b6a71d54e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T_B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1347034/1542894672.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbaby_logits_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaby_logits_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mchina_logits_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchina_logits_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mT_B\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_C\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T_B' is not defined"
     ]
    }
   ],
   "source": [
    "Xg, yg = build_gate_dataset(\n",
    "    X_baby=X_test0,\n",
    "    X_china=X_val_Chinese_babyCry,\n",
    "    baby_logits_model=baby_logits_model,\n",
    "    china_logits_model=china_logits_model,\n",
    "    T_B=T_B, T_C=T_C\n",
    ")\n",
    "\n",
    "gate_mlp = train_domain_gate_mlp(Xg, yg, epochs=200, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff0c1926-130f-4df3-9c57-c56740a28e9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gate_mlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1347034/2952525796.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbaby_logits_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaby_logits_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mchina_logits_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchina_logits_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgate_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgate_mlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mT_B\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_C\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_C\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gate_mlp' is not defined"
     ]
    }
   ],
   "source": [
    "# --- gate behavior on baby test ---\n",
    "zB_b, zC_b, wb_b, wc_b, pch_b, ydom_b = eval_gate_on_dataset(\n",
    "    X=X_test0,\n",
    "    true_domain=0,\n",
    "    baby_logits_model=baby_logits_model,\n",
    "    china_logits_model=china_logits_model,\n",
    "    gate_model=gate_mlp,\n",
    "    T_B=T_B, T_C=T_C,\n",
    "    alpha=2.0,\n",
    "    name=\"BABY test\"\n",
    ")\n",
    "\n",
    "# --- fuse open-world ---\n",
    "p_union_b, y_pred_union_b = fuse_union_openworld(\n",
    "    baby_logits_3=zB_b, china_logits_3=zC_b,\n",
    "    T_B=T_B, T_C=T_C,\n",
    "    wb=wb_b, wc=wc_b\n",
    ")\n",
    "\n",
    "# --- map baby true labels into union ---\n",
    "y_true_union_b = map_baby_y_to_union(y_test0)\n",
    "\n",
    "# evaluate only on baby subset labels\n",
    "baby_metrics = eval_ensemble_subset(\n",
    "    y_true_union_b, y_pred_union_b,\n",
    "    subset_union_ids=BABY_UNION_IDS,\n",
    "    subset_names=BABY_LABELS_3,\n",
    "    name=\"Ensemble on BABY test (subset=Baby classes)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc69cc2e-1ce6-451d-ae34-99a8b1ac7cff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gate_mlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1347034/738658339.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbaby_logits_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaby_logits_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mchina_logits_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchina_logits_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgate_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgate_mlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mT_B\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_C\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_C\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gate_mlp' is not defined"
     ]
    }
   ],
   "source": [
    "# --- gate behavior on china test ---\n",
    "zB_c, zC_c, wb_c, wc_c, pch_c, ydom_c = eval_gate_on_dataset(\n",
    "    X=X_val_Chinese_babyCry,\n",
    "    true_domain=1,\n",
    "    baby_logits_model=baby_logits_model,\n",
    "    china_logits_model=china_logits_model,\n",
    "    gate_model=gate_mlp,\n",
    "    T_B=T_B, T_C=T_C,\n",
    "    alpha=2.0,\n",
    "    name=\"CHINA test\"\n",
    ")\n",
    "\n",
    "# --- fuse open-world ---\n",
    "p_union_c, y_pred_union_c = fuse_union_openworld(\n",
    "    baby_logits_3=zB_c, china_logits_3=zC_c,\n",
    "    T_B=T_B, T_C=T_C,\n",
    "    wb=wb_c, wc=wc_c\n",
    ")\n",
    "\n",
    "# --- map china true labels into union ---\n",
    "y_true_union_c = map_china_y_to_union(y_val_Chinese_babyCry)\n",
    "\n",
    "# evaluate only on china subset labels\n",
    "china_metrics = eval_ensemble_subset(\n",
    "    y_true_union_c, y_pred_union_c,\n",
    "    subset_union_ids=CHINA_UNION_IDS,\n",
    "    subset_names=CHINA_LABELS_3,\n",
    "    name=\"Ensemble on CHINA test (subset=China classes)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284337e2-2fd3-45d8-9c85-5441cf1c047d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28669ce5-3901-4777-9a93-79c35e7afa54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ba47b-c7ed-4118-8b88-db0faa1965ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2412886-6952-4581-90fa-4c53e04a5cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e727aec-f2f4-464e-b8d8-3bc2c00614ce",
   "metadata": {},
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fee597e-3ffd-4833-96d0-f7329a1d10b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ddf5d-0db7-4134-a656-bdff50dc9345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bfe64c-c31b-4b03-ae75-b1de1c02dac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2191d-c247-4381-9005-248746a28520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfe094cc-43b7-42bb-8e2e-2f0edaf3f7fe",
   "metadata": {},
   "source": [
    "# Entropy-gated fusion on the union label space\n",
    "\n",
    "### Implements exactly:\n",
    "\n",
    "### calibrated posteriors p_m(y|x,T_m)\n",
    "\n",
    "### union logits initialized to -inf\n",
    "\n",
    "### fill disjoint classes directly\n",
    "\n",
    "### fuse shared sleepy with log-sum-exp with weights\n",
    "\n",
    "### weights from entropy: \n",
    "𝑤\n",
    "𝑚\n",
    "∝\n",
    "exp\n",
    "⁡\n",
    "(\n",
    "−\n",
    "𝜏\n",
    "𝐻\n",
    "(\n",
    "𝑝\n",
    "𝑚\n",
    ")\n",
    ")\n",
    "w\n",
    "m​\n",
    "∝exp(−τH(p\n",
    "m\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ba055d5-7709-442c-aeb1-f298919e002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p, eps=1e-12):\n",
    "    # p: [C]\n",
    "    p = torch.clamp(p, eps, 1.0)\n",
    "    return -(p * torch.log(p)).sum()\n",
    "\n",
    "@torch.no_grad()\n",
    "def calibrated_probs(model, x, T, device):\n",
    "    model.eval()\n",
    "    x = x.to(device)\n",
    "    logits = model(x)  # [B, C]\n",
    "    probs = F.softmax(logits / T, dim=-1)\n",
    "    return logits, probs\n",
    "\n",
    "@torch.no_grad()\n",
    "def fuse_two_models_union(\n",
    "    x, baby_model, china_model, T_B, T_C,\n",
    "    tau=1.0, device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    x: [B, ...] batch\n",
    "    Returns:\n",
    "      p_star: [B, |UNION|]\n",
    "      z_union: [B, |UNION|] union logits pre-softmax\n",
    "      weights: dict with wB, wC per sample\n",
    "    \"\"\"\n",
    "    # Get calibrated logits & probs\n",
    "    zB, pB = calibrated_probs(baby_model, x, T_B, device)    # [B,3]\n",
    "    zC, pC = calibrated_probs(china_model, x, T_C, device)   # [B,3]\n",
    "\n",
    "    B = x.shape[0]\n",
    "    z_union = torch.full((B, len(UNION_LABELS)), -1e9, device=device)  # approx -inf\n",
    "\n",
    "    # Entropy gated weights per sample\n",
    "    wB_list, wC_list = [], []\n",
    "    for i in range(B):\n",
    "        HB = entropy(pB[i])\n",
    "        HC = entropy(pC[i])\n",
    "        aB = torch.exp(-tau * HB)\n",
    "        aC = torch.exp(-tau * HC)\n",
    "        s = aB + aC\n",
    "        wB = aB / s\n",
    "        wC = aC / s\n",
    "        wB_list.append(wB)\n",
    "        wC_list.append(wC)\n",
    "\n",
    "    wB = torch.stack(wB_list).to(device)  # [B]\n",
    "    wC = torch.stack(wC_list).to(device)  # [B]\n",
    "\n",
    "    # Fill disjoint labels into union logits:\n",
    "    # Baby2020 contributes: hug, uncomfortable, sleepy\n",
    "    z_union[:, union2idx[\"hug\"]] = zB[:, baby2idx[\"hug\"]]\n",
    "    z_union[:, union2idx[\"uncomfortable\"]] = zB[:, baby2idx[\"uncomfortable\"]]\n",
    "\n",
    "    # Chinese contributes: hungry, awake, sleepy\n",
    "    z_union[:, union2idx[\"hungry\"]] = zC[:, china2idx[\"hungry\"]]\n",
    "    z_union[:, union2idx[\"awake\"]]  = zC[:, china2idx[\"awake\"]]\n",
    "\n",
    "    # Fuse shared class \"sleepy\" using: log( wC * exp(zC_sleepy) + wB * exp(zB_sleepy) )\n",
    "    z_sleepy = torch.log(\n",
    "        wC * torch.exp(zC[:, CHINA_SLEEPY_IDX]) + wB * torch.exp(zB[:, BABY_SLEEPY_IDX])\n",
    "    )\n",
    "    z_union[:, UNION_SLEEPY_IDX] = z_sleepy\n",
    "\n",
    "    # Final posterior over union\n",
    "    p_star = F.softmax(z_union, dim=-1)\n",
    "\n",
    "    return p_star, z_union, {\"wB\": wB, \"wC\": wC}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0087e37-2c18-4d27-8bf5-8e2c0fdc21eb",
   "metadata": {},
   "source": [
    "# Run fusion on a test loader (example)\n",
    "\n",
    "### Your test loader should yield (xb, y_union) where y_union is indexed in UNION_LABELS.\n",
    "### If you don’t have union-ground-truth for both datasets, evaluate per test set accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0896e468-e759-4b7a-967e-87c244479a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_union_fusion(test_loader, tau=1.0, device=\"cpu\"):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for xb, y_union in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        y_union = y_union.to(device)\n",
    "\n",
    "        p_star, z_union, w = fuse_two_models_union(\n",
    "            xb, baby_model, china_model, T_B, T_C, tau=tau, device=device\n",
    "        )\n",
    "        pred = torch.argmax(p_star, dim=-1)\n",
    "        correct += (pred == y_union).sum().item()\n",
    "        total += y_union.numel()\n",
    "\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "# acc = evaluate_union_fusion(union_test_loader, tau=1.0, device=device)\n",
    "# print(\"Fusion accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a82b3-2ec5-45cb-aa5b-ff38731f5141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "/usr/bin/python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
